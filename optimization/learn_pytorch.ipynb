{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-model-optimization in /Users/calvinatewamba/anaconda3/envs/keras_env/lib/python3.10/site-packages (0.7.5)\n",
      "Requirement already satisfied: absl-py~=1.2 in /Users/calvinatewamba/anaconda3/envs/keras_env/lib/python3.10/site-packages (from tensorflow-model-optimization) (1.4.0)\n",
      "Requirement already satisfied: dm-tree~=0.1.1 in /Users/calvinatewamba/anaconda3/envs/keras_env/lib/python3.10/site-packages (from tensorflow-model-optimization) (0.1.8)\n",
      "Requirement already satisfied: numpy~=1.23 in /Users/calvinatewamba/anaconda3/envs/keras_env/lib/python3.10/site-packages (from tensorflow-model-optimization) (1.23.5)\n",
      "Requirement already satisfied: six~=1.14 in /Users/calvinatewamba/anaconda3/envs/keras_env/lib/python3.10/site-packages (from tensorflow-model-optimization) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "# Write python code to specify, train, evaluate and deploy a neural network with three layers using pytorch\n",
    "!pip3 install tensorflow-model-optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# Generate the training and test data\n",
    "x_train=np.random.normal(0,5,size=(300000,200)).astype('float32')\n",
    "x_test=np.random.normal(0,5,size=(10000,200)).astype('float32')\n",
    "print(type(x_train))\n",
    "\n",
    "def get_targets(x:np.ndarray)->int:\n",
    "    x_sum=np.sum(x)\n",
    "    target=(0 if x_sum<=-100 else 1 if -100<x_sum<=-75 else 2 if -75<x_sum<=-50\n",
    "            else 3 if -50<x_sum<=-25 else 4 if -25<x_sum<=0 else 5 if 0<x_sum<=25\n",
    "            else 6 if 25<x_sum<=50 else 7 if 50<x_sum<=75 else 8 if 75<x_sum<=100\n",
    "            else 9)\n",
    "    return target\n",
    "\n",
    "y_train=np.array([get_targets(row) for row in x_train])\n",
    "y_test=np.array([get_targets(row) for row in x_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data into dataloader\n",
    "train_data=[(torch.tensor(row1), torch.tensor(row2)) for row1, row2 in zip(x_train, y_train)]\n",
    "test_data=[(torch.tensor(row1), torch.tensor(row2)) for row1, row2 in zip(x_test, y_test)]\n",
    "\n",
    "batch_size=64\n",
    "train_dataloader=DataLoader(train_data, batch_size=batch_size)\n",
    "test_dataloader=DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape:torch.Size([64, 200])\n",
      "y shape:torch.Size([64])\n",
      "train dataset size:100000\n",
      "number of batches:1563\n"
     ]
    }
   ],
   "source": [
    "for X,y in train_dataloader:\n",
    "    print(f\"X shape:{X.shape}\")\n",
    "    print(f\"y shape:{y.shape}\")\n",
    "    break\n",
    "\n",
    "print(f\"train dataset size:{len(train_dataloader.dataset)}\")\n",
    "print(f\"number of batches:{len(train_dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (normalizer): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=200, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=128, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Write the model\n",
    "\n",
    "device=(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.normalizer=nn.BatchNorm1d(200)\n",
    "        self.linear_relu_stack=nn.Sequential(\n",
    "            nn.Linear(200, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,10)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x=self.normalizer(x)\n",
    "        logits=self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model=NeuralNetwork().to(device)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss and the optimizer\n",
    "loss_fn=nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the train and test process\n",
    "def train(model, dataloader, loss_fn, optimizer):\n",
    "    size=len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X,y=X.to(device), y.to(device)\n",
    "        pred=model(X)\n",
    "        loss=loss_fn(pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch %100==0:\n",
    "            loss,current=loss.item(),(batch+1)*len(X)\n",
    "            print(f\"loss:{loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test(model, dataloader, loss_fn):\n",
    "    size=len(dataloader.dataset)\n",
    "    num_batches=len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, true_values=0.00, 0\n",
    "    with torch.no_grad():\n",
    "        for X,y in dataloader:\n",
    "            X,y=X.to(device), y.to(device)\n",
    "            pred=model(X)\n",
    "            loss=loss_fn(pred, y)\n",
    "            test_loss+=loss\n",
    "            true_values+=(pred.argmax(1)==y).type(torch.float).sum().item()\n",
    "        test_loss/=num_batches\n",
    "        true_values/=size\n",
    "        print(f\"Test\\n Avg Loss:{test_loss:>7f}  Accuracy:{100*true_values:>0.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch1----------------\n",
      "\n",
      "loss:2.290251  [   64/100000]\n",
      "loss:1.230391  [ 6464/100000]\n",
      "loss:0.978134  [12864/100000]\n",
      "loss:0.762810  [19264/100000]\n",
      "loss:0.937361  [25664/100000]\n",
      "loss:1.256884  [32064/100000]\n",
      "loss:0.819451  [38464/100000]\n",
      "loss:0.806127  [44864/100000]\n",
      "loss:1.472703  [51264/100000]\n",
      "loss:0.745594  [57664/100000]\n",
      "loss:0.761159  [64064/100000]\n",
      "loss:0.685097  [70464/100000]\n",
      "loss:0.748183  [76864/100000]\n",
      "loss:0.774111  [83264/100000]\n",
      "loss:0.752388  [89664/100000]\n",
      "loss:0.848142  [96064/100000]\n",
      "Test\n",
      " Avg Loss:0.848421  Accuracy:62.5\n",
      "Epoch2----------------\n",
      "\n",
      "loss:1.099652  [   64/100000]\n",
      "loss:0.707739  [ 6464/100000]\n",
      "loss:1.050079  [12864/100000]\n",
      "loss:0.592082  [19264/100000]\n",
      "loss:0.763296  [25664/100000]\n",
      "loss:1.312450  [32064/100000]\n",
      "loss:0.701744  [38464/100000]\n",
      "loss:0.682152  [44864/100000]\n",
      "loss:1.444980  [51264/100000]\n",
      "loss:0.633581  [57664/100000]\n",
      "loss:0.655443  [64064/100000]\n",
      "loss:0.597528  [70464/100000]\n",
      "loss:0.690203  [76864/100000]\n",
      "loss:0.704943  [83264/100000]\n",
      "loss:0.784809  [89664/100000]\n",
      "loss:0.729485  [96064/100000]\n",
      "Test\n",
      " Avg Loss:0.697374  Accuracy:70.7\n",
      "Epoch3----------------\n",
      "\n",
      "loss:0.837790  [   64/100000]\n",
      "loss:0.674890  [ 6464/100000]\n",
      "loss:1.110570  [12864/100000]\n",
      "loss:0.539088  [19264/100000]\n",
      "loss:0.777881  [25664/100000]\n",
      "loss:1.320582  [32064/100000]\n",
      "loss:0.631148  [38464/100000]\n",
      "loss:0.652103  [44864/100000]\n",
      "loss:1.454155  [51264/100000]\n",
      "loss:0.626671  [57664/100000]\n",
      "loss:0.594221  [64064/100000]\n",
      "loss:0.554647  [70464/100000]\n",
      "loss:0.672721  [76864/100000]\n",
      "loss:0.710775  [83264/100000]\n",
      "loss:0.766211  [89664/100000]\n",
      "loss:0.695749  [96064/100000]\n",
      "Test\n",
      " Avg Loss:0.606969  Accuracy:75.1\n",
      "Epoch4----------------\n",
      "\n",
      "loss:0.704967  [   64/100000]\n",
      "loss:0.648709  [ 6464/100000]\n",
      "loss:1.124009  [12864/100000]\n",
      "loss:0.513664  [19264/100000]\n",
      "loss:0.751577  [25664/100000]\n",
      "loss:1.271192  [32064/100000]\n",
      "loss:0.547830  [38464/100000]\n",
      "loss:0.637933  [44864/100000]\n",
      "loss:1.268223  [51264/100000]\n",
      "loss:0.624039  [57664/100000]\n",
      "loss:0.584552  [64064/100000]\n",
      "loss:0.542671  [70464/100000]\n",
      "loss:0.651389  [76864/100000]\n",
      "loss:0.710674  [83264/100000]\n",
      "loss:0.712270  [89664/100000]\n",
      "loss:0.687457  [96064/100000]\n",
      "Test\n",
      " Avg Loss:0.561708  Accuracy:77.8\n",
      "Epoch5----------------\n",
      "\n",
      "loss:0.633638  [   64/100000]\n",
      "loss:0.634481  [ 6464/100000]\n",
      "loss:1.076427  [12864/100000]\n",
      "loss:0.498963  [19264/100000]\n",
      "loss:0.733226  [25664/100000]\n",
      "loss:1.194583  [32064/100000]\n",
      "loss:0.512639  [38464/100000]\n",
      "loss:0.599962  [44864/100000]\n",
      "loss:1.114219  [51264/100000]\n",
      "loss:0.597043  [57664/100000]\n",
      "loss:0.571451  [64064/100000]\n",
      "loss:0.539664  [70464/100000]\n",
      "loss:0.601193  [76864/100000]\n",
      "loss:0.714738  [83264/100000]\n",
      "loss:0.712311  [89664/100000]\n",
      "loss:0.664917  [96064/100000]\n",
      "Test\n",
      " Avg Loss:0.551838  Accuracy:77.8\n",
      "Epoch6----------------\n",
      "\n",
      "loss:0.600009  [   64/100000]\n",
      "loss:0.617968  [ 6464/100000]\n",
      "loss:0.964422  [12864/100000]\n",
      "loss:0.497854  [19264/100000]\n",
      "loss:0.665895  [25664/100000]\n",
      "loss:1.089553  [32064/100000]\n",
      "loss:0.493079  [38464/100000]\n",
      "loss:0.571721  [44864/100000]\n",
      "loss:0.966071  [51264/100000]\n",
      "loss:0.540899  [57664/100000]\n",
      "loss:0.592099  [64064/100000]\n",
      "loss:0.561459  [70464/100000]\n",
      "loss:0.587964  [76864/100000]\n",
      "loss:0.692674  [83264/100000]\n",
      "loss:0.712906  [89664/100000]\n",
      "loss:0.643782  [96064/100000]\n",
      "Test\n",
      " Avg Loss:0.555160  Accuracy:77.0\n",
      "Epoch7----------------\n",
      "\n",
      "loss:0.588656  [   64/100000]\n",
      "loss:0.623058  [ 6464/100000]\n",
      "loss:0.831958  [12864/100000]\n",
      "loss:0.477501  [19264/100000]\n",
      "loss:0.641023  [25664/100000]\n",
      "loss:1.120764  [32064/100000]\n",
      "loss:0.479468  [38464/100000]\n",
      "loss:0.560435  [44864/100000]\n",
      "loss:0.874027  [51264/100000]\n",
      "loss:0.488461  [57664/100000]\n",
      "loss:0.609661  [64064/100000]\n",
      "loss:0.560191  [70464/100000]\n",
      "loss:0.572710  [76864/100000]\n",
      "loss:0.694624  [83264/100000]\n",
      "loss:0.721914  [89664/100000]\n",
      "loss:0.624287  [96064/100000]\n",
      "Test\n",
      " Avg Loss:0.563390  Accuracy:76.4\n",
      "Epoch8----------------\n",
      "\n",
      "loss:0.570154  [   64/100000]\n",
      "loss:0.635557  [ 6464/100000]\n",
      "loss:0.773534  [12864/100000]\n",
      "loss:0.480685  [19264/100000]\n",
      "loss:0.598335  [25664/100000]\n",
      "loss:1.139687  [32064/100000]\n",
      "loss:0.477673  [38464/100000]\n",
      "loss:0.548051  [44864/100000]\n",
      "loss:0.850759  [51264/100000]\n",
      "loss:0.478272  [57664/100000]\n",
      "loss:0.580715  [64064/100000]\n",
      "loss:0.571919  [70464/100000]\n",
      "loss:0.549864  [76864/100000]\n",
      "loss:0.660694  [83264/100000]\n",
      "loss:0.702816  [89664/100000]\n",
      "loss:0.610464  [96064/100000]\n",
      "Test\n",
      " Avg Loss:0.573119  Accuracy:75.8\n",
      "Epoch9----------------\n",
      "\n",
      "loss:0.521289  [   64/100000]\n",
      "loss:0.631060  [ 6464/100000]\n",
      "loss:0.732937  [12864/100000]\n",
      "loss:0.463130  [19264/100000]\n",
      "loss:0.553332  [25664/100000]\n",
      "loss:1.164161  [32064/100000]\n",
      "loss:0.473827  [38464/100000]\n",
      "loss:0.532399  [44864/100000]\n",
      "loss:0.853494  [51264/100000]\n",
      "loss:0.466724  [57664/100000]\n",
      "loss:0.581720  [64064/100000]\n",
      "loss:0.565480  [70464/100000]\n",
      "loss:0.563306  [76864/100000]\n",
      "loss:0.630312  [83264/100000]\n",
      "loss:0.665845  [89664/100000]\n",
      "loss:0.589198  [96064/100000]\n",
      "Test\n",
      " Avg Loss:0.588847  Accuracy:74.5\n",
      "Epoch10----------------\n",
      "\n",
      "loss:0.492345  [   64/100000]\n",
      "loss:0.635496  [ 6464/100000]\n",
      "loss:0.652884  [12864/100000]\n",
      "loss:0.449925  [19264/100000]\n",
      "loss:0.549309  [25664/100000]\n",
      "loss:1.148316  [32064/100000]\n",
      "loss:0.435217  [38464/100000]\n",
      "loss:0.495578  [44864/100000]\n",
      "loss:0.808793  [51264/100000]\n",
      "loss:0.416901  [57664/100000]\n",
      "loss:0.528442  [64064/100000]\n",
      "loss:0.562672  [70464/100000]\n",
      "loss:0.555258  [76864/100000]\n",
      "loss:0.630944  [83264/100000]\n",
      "loss:0.630072  [89664/100000]\n",
      "loss:0.573034  [96064/100000]\n",
      "Test\n",
      " Avg Loss:0.589354  Accuracy:74.5\n",
      "Epoch11----------------\n",
      "\n",
      "loss:0.464097  [   64/100000]\n",
      "loss:0.632295  [ 6464/100000]\n",
      "loss:0.648170  [12864/100000]\n",
      "loss:0.453929  [19264/100000]\n",
      "loss:0.517364  [25664/100000]\n",
      "loss:1.127550  [32064/100000]\n",
      "loss:0.435163  [38464/100000]\n",
      "loss:0.482915  [44864/100000]\n",
      "loss:0.786786  [51264/100000]\n",
      "loss:0.389060  [57664/100000]\n",
      "loss:0.467179  [64064/100000]\n",
      "loss:0.575270  [70464/100000]\n",
      "loss:0.534572  [76864/100000]\n",
      "loss:0.612848  [83264/100000]\n",
      "loss:0.563837  [89664/100000]\n",
      "loss:0.547433  [96064/100000]\n",
      "Test\n",
      " Avg Loss:0.610753  Accuracy:73.7\n",
      "Epoch12----------------\n",
      "\n",
      "loss:0.437564  [   64/100000]\n",
      "loss:0.602063  [ 6464/100000]\n",
      "loss:0.590985  [12864/100000]\n",
      "loss:0.422851  [19264/100000]\n",
      "loss:0.498865  [25664/100000]\n",
      "loss:1.117298  [32064/100000]\n",
      "loss:0.389132  [38464/100000]\n",
      "loss:0.491576  [44864/100000]\n",
      "loss:0.813982  [51264/100000]\n",
      "loss:0.384170  [57664/100000]\n",
      "loss:0.450113  [64064/100000]\n",
      "loss:0.574746  [70464/100000]\n",
      "loss:0.513858  [76864/100000]\n",
      "loss:0.599697  [83264/100000]\n",
      "loss:0.531884  [89664/100000]\n",
      "loss:0.574589  [96064/100000]\n",
      "Test\n",
      " Avg Loss:0.628885  Accuracy:72.9\n",
      "Epoch13----------------\n",
      "\n",
      "loss:0.412778  [   64/100000]\n",
      "loss:0.591120  [ 6464/100000]\n",
      "loss:0.547676  [12864/100000]\n",
      "loss:0.394600  [19264/100000]\n",
      "loss:0.460077  [25664/100000]\n",
      "loss:1.058275  [32064/100000]\n",
      "loss:0.375180  [38464/100000]\n",
      "loss:0.486240  [44864/100000]\n",
      "loss:0.744972  [51264/100000]\n",
      "loss:0.361646  [57664/100000]\n",
      "loss:0.422225  [64064/100000]\n",
      "loss:0.598897  [70464/100000]\n",
      "loss:0.536861  [76864/100000]\n",
      "loss:0.574093  [83264/100000]\n",
      "loss:0.547083  [89664/100000]\n",
      "loss:0.548344  [96064/100000]\n",
      "Test\n",
      " Avg Loss:0.652880  Accuracy:72.1\n",
      "Epoch14----------------\n",
      "\n",
      "loss:0.394199  [   64/100000]\n",
      "loss:0.539622  [ 6464/100000]\n",
      "loss:0.498933  [12864/100000]\n",
      "loss:0.406087  [19264/100000]\n",
      "loss:0.419406  [25664/100000]\n",
      "loss:1.008729  [32064/100000]\n",
      "loss:0.394919  [38464/100000]\n",
      "loss:0.497249  [44864/100000]\n",
      "loss:0.725731  [51264/100000]\n",
      "loss:0.371600  [57664/100000]\n",
      "loss:0.398120  [64064/100000]\n",
      "loss:0.578546  [70464/100000]\n",
      "loss:0.540100  [76864/100000]\n",
      "loss:0.532390  [83264/100000]\n",
      "loss:0.563345  [89664/100000]\n",
      "loss:0.534513  [96064/100000]\n",
      "Test\n",
      " Avg Loss:0.669876  Accuracy:71.5\n",
      "Epoch15----------------\n",
      "\n",
      "loss:0.392747  [   64/100000]\n",
      "loss:0.508600  [ 6464/100000]\n",
      "loss:0.501545  [12864/100000]\n",
      "loss:0.391401  [19264/100000]\n",
      "loss:0.433002  [25664/100000]\n",
      "loss:0.989094  [32064/100000]\n",
      "loss:0.366291  [38464/100000]\n",
      "loss:0.516959  [44864/100000]\n",
      "loss:0.707285  [51264/100000]\n",
      "loss:0.374491  [57664/100000]\n",
      "loss:0.401936  [64064/100000]\n",
      "loss:0.558196  [70464/100000]\n",
      "loss:0.504617  [76864/100000]\n",
      "loss:0.567042  [83264/100000]\n",
      "loss:0.473987  [89664/100000]\n",
      "loss:0.542551  [96064/100000]\n",
      "Test\n",
      " Avg Loss:0.690310  Accuracy:71.4\n",
      "Epoch16----------------\n",
      "\n",
      "loss:0.371076  [   64/100000]\n",
      "loss:0.494047  [ 6464/100000]\n",
      "loss:0.490558  [12864/100000]\n",
      "loss:0.354815  [19264/100000]\n",
      "loss:0.418381  [25664/100000]\n",
      "loss:1.022584  [32064/100000]\n",
      "loss:0.366914  [38464/100000]\n",
      "loss:0.503292  [44864/100000]\n",
      "loss:0.739935  [51264/100000]\n",
      "loss:0.334187  [57664/100000]\n",
      "loss:0.391894  [64064/100000]\n",
      "loss:0.558469  [70464/100000]\n",
      "loss:0.467852  [76864/100000]\n",
      "loss:0.531990  [83264/100000]\n",
      "loss:0.483929  [89664/100000]\n",
      "loss:0.491528  [96064/100000]\n",
      "Test\n",
      " Avg Loss:0.704395  Accuracy:71.5\n",
      "Epoch17----------------\n",
      "\n",
      "loss:0.350003  [   64/100000]\n",
      "loss:0.451197  [ 6464/100000]\n",
      "loss:0.423612  [12864/100000]\n",
      "loss:0.373547  [19264/100000]\n",
      "loss:0.408043  [25664/100000]\n",
      "loss:1.001268  [32064/100000]\n",
      "loss:0.362997  [38464/100000]\n",
      "loss:0.522773  [44864/100000]\n",
      "loss:0.682798  [51264/100000]\n",
      "loss:0.351024  [57664/100000]\n",
      "loss:0.343622  [64064/100000]\n",
      "loss:0.521291  [70464/100000]\n",
      "loss:0.468137  [76864/100000]\n",
      "loss:0.550304  [83264/100000]\n",
      "loss:0.420550  [89664/100000]\n",
      "loss:0.441556  [96064/100000]\n",
      "Test\n",
      " Avg Loss:0.747019  Accuracy:70.1\n",
      "Epoch18----------------\n",
      "\n",
      "loss:0.358954  [   64/100000]\n",
      "loss:0.440566  [ 6464/100000]\n",
      "loss:0.452752  [12864/100000]\n",
      "loss:0.447159  [19264/100000]\n",
      "loss:0.353050  [25664/100000]\n",
      "loss:0.951743  [32064/100000]\n",
      "loss:0.346036  [38464/100000]\n",
      "loss:0.462275  [44864/100000]\n",
      "loss:0.714806  [51264/100000]\n",
      "loss:0.325887  [57664/100000]\n",
      "loss:0.345473  [64064/100000]\n",
      "loss:0.567237  [70464/100000]\n",
      "loss:0.394121  [76864/100000]\n",
      "loss:0.582161  [83264/100000]\n",
      "loss:0.427906  [89664/100000]\n",
      "loss:0.479778  [96064/100000]\n",
      "Test\n",
      " Avg Loss:0.771338  Accuracy:69.6\n",
      "Epoch19----------------\n",
      "\n",
      "loss:0.389395  [   64/100000]\n",
      "loss:0.429498  [ 6464/100000]\n",
      "loss:0.383693  [12864/100000]\n",
      "loss:0.351677  [19264/100000]\n",
      "loss:0.362165  [25664/100000]\n",
      "loss:1.012528  [32064/100000]\n",
      "loss:0.367813  [38464/100000]\n",
      "loss:0.435754  [44864/100000]\n",
      "loss:0.609999  [51264/100000]\n",
      "loss:0.331852  [57664/100000]\n",
      "loss:0.331284  [64064/100000]\n",
      "loss:0.535704  [70464/100000]\n",
      "loss:0.422177  [76864/100000]\n",
      "loss:0.523213  [83264/100000]\n",
      "loss:0.443407  [89664/100000]\n",
      "loss:0.466046  [96064/100000]\n",
      "Test\n",
      " Avg Loss:0.764795  Accuracy:69.4\n",
      "Epoch20----------------\n",
      "\n",
      "loss:0.305079  [   64/100000]\n",
      "loss:0.439536  [ 6464/100000]\n",
      "loss:0.394658  [12864/100000]\n",
      "loss:0.358853  [19264/100000]\n",
      "loss:0.337500  [25664/100000]\n",
      "loss:0.916571  [32064/100000]\n",
      "loss:0.400859  [38464/100000]\n",
      "loss:0.409508  [44864/100000]\n",
      "loss:0.636472  [51264/100000]\n",
      "loss:0.305799  [57664/100000]\n",
      "loss:0.326081  [64064/100000]\n",
      "loss:0.530230  [70464/100000]\n",
      "loss:0.397165  [76864/100000]\n",
      "loss:0.545576  [83264/100000]\n",
      "loss:0.378239  [89664/100000]\n",
      "loss:0.584898  [96064/100000]\n",
      "Test\n",
      " Avg Loss:0.810688  Accuracy:69.0\n",
      "Epoch21----------------\n",
      "\n",
      "loss:0.316615  [   64/100000]\n",
      "loss:0.415501  [ 6464/100000]\n",
      "loss:0.370674  [12864/100000]\n",
      "loss:0.296115  [19264/100000]\n",
      "loss:0.307192  [25664/100000]\n",
      "loss:0.858195  [32064/100000]\n",
      "loss:0.372504  [38464/100000]\n",
      "loss:0.392452  [44864/100000]\n",
      "loss:0.578751  [51264/100000]\n",
      "loss:0.313503  [57664/100000]\n",
      "loss:0.319331  [64064/100000]\n",
      "loss:0.509271  [70464/100000]\n",
      "loss:0.388103  [76864/100000]\n",
      "loss:0.538372  [83264/100000]\n",
      "loss:0.440749  [89664/100000]\n",
      "loss:0.414298  [96064/100000]\n",
      "Test\n",
      " Avg Loss:0.836277  Accuracy:68.8\n",
      "Epoch22----------------\n",
      "\n",
      "loss:0.325731  [   64/100000]\n",
      "loss:0.392330  [ 6464/100000]\n",
      "loss:0.427199  [12864/100000]\n",
      "loss:0.272328  [19264/100000]\n",
      "loss:0.290093  [25664/100000]\n",
      "loss:0.908440  [32064/100000]\n",
      "loss:0.376890  [38464/100000]\n",
      "loss:0.337832  [44864/100000]\n",
      "loss:0.613527  [51264/100000]\n",
      "loss:0.296930  [57664/100000]\n",
      "loss:0.322463  [64064/100000]\n",
      "loss:0.530428  [70464/100000]\n",
      "loss:0.360934  [76864/100000]\n",
      "loss:0.445334  [83264/100000]\n",
      "loss:0.407346  [89664/100000]\n",
      "loss:0.404132  [96064/100000]\n",
      "Test\n",
      " Avg Loss:0.859069  Accuracy:68.7\n",
      "Epoch23----------------\n",
      "\n",
      "loss:0.311300  [   64/100000]\n",
      "loss:0.410673  [ 6464/100000]\n",
      "loss:0.351770  [12864/100000]\n",
      "loss:0.306310  [19264/100000]\n",
      "loss:0.274527  [25664/100000]\n",
      "loss:0.903974  [32064/100000]\n",
      "loss:0.368690  [38464/100000]\n",
      "loss:0.357418  [44864/100000]\n",
      "loss:0.539558  [51264/100000]\n",
      "loss:0.301822  [57664/100000]\n",
      "loss:0.317722  [64064/100000]\n",
      "loss:0.514405  [70464/100000]\n",
      "loss:0.357118  [76864/100000]\n",
      "loss:0.399901  [83264/100000]\n",
      "loss:0.362152  [89664/100000]\n",
      "loss:0.417575  [96064/100000]\n",
      "Test\n",
      " Avg Loss:0.895587  Accuracy:68.3\n",
      "Epoch24----------------\n",
      "\n",
      "loss:0.309005  [   64/100000]\n",
      "loss:0.379461  [ 6464/100000]\n",
      "loss:0.322275  [12864/100000]\n",
      "loss:0.306181  [19264/100000]\n",
      "loss:0.320531  [25664/100000]\n",
      "loss:0.847902  [32064/100000]\n",
      "loss:0.426628  [38464/100000]\n",
      "loss:0.379962  [44864/100000]\n",
      "loss:0.515391  [51264/100000]\n",
      "loss:0.355564  [57664/100000]\n",
      "loss:0.311534  [64064/100000]\n",
      "loss:0.475218  [70464/100000]\n",
      "loss:0.332579  [76864/100000]\n",
      "loss:0.412437  [83264/100000]\n",
      "loss:0.327391  [89664/100000]\n",
      "loss:0.435377  [96064/100000]\n",
      "Test\n",
      " Avg Loss:0.892801  Accuracy:68.1\n",
      "Epoch25----------------\n",
      "\n",
      "loss:0.346894  [   64/100000]\n",
      "loss:0.371673  [ 6464/100000]\n",
      "loss:0.335481  [12864/100000]\n",
      "loss:0.341764  [19264/100000]\n",
      "loss:0.257301  [25664/100000]\n",
      "loss:0.866910  [32064/100000]\n",
      "loss:0.365811  [38464/100000]\n",
      "loss:0.389231  [44864/100000]\n",
      "loss:0.506882  [51264/100000]\n",
      "loss:0.295523  [57664/100000]\n",
      "loss:0.301971  [64064/100000]\n",
      "loss:0.475317  [70464/100000]\n",
      "loss:0.320398  [76864/100000]\n",
      "loss:0.363579  [83264/100000]\n",
      "loss:0.328938  [89664/100000]\n",
      "loss:0.409345  [96064/100000]\n",
      "Test\n",
      " Avg Loss:0.929089  Accuracy:68.2\n",
      "Epoch26----------------\n",
      "\n",
      "loss:0.238035  [   64/100000]\n",
      "loss:0.415129  [ 6464/100000]\n",
      "loss:0.244394  [12864/100000]\n",
      "loss:0.277862  [19264/100000]\n",
      "loss:0.256996  [25664/100000]\n",
      "loss:0.981448  [32064/100000]\n",
      "loss:0.394728  [38464/100000]\n",
      "loss:0.306933  [44864/100000]\n",
      "loss:0.525440  [51264/100000]\n",
      "loss:0.294697  [57664/100000]\n",
      "loss:0.247267  [64064/100000]\n",
      "loss:0.398205  [70464/100000]\n",
      "loss:0.345617  [76864/100000]\n",
      "loss:0.398119  [83264/100000]\n",
      "loss:0.289104  [89664/100000]\n",
      "loss:0.432200  [96064/100000]\n",
      "Test\n",
      " Avg Loss:0.996228  Accuracy:67.3\n",
      "Epoch27----------------\n",
      "\n",
      "loss:0.254380  [   64/100000]\n",
      "loss:0.376830  [ 6464/100000]\n",
      "loss:0.278320  [12864/100000]\n",
      "loss:0.277958  [19264/100000]\n",
      "loss:0.235741  [25664/100000]\n",
      "loss:0.799519  [32064/100000]\n",
      "loss:0.373346  [38464/100000]\n",
      "loss:0.345022  [44864/100000]\n",
      "loss:0.615135  [51264/100000]\n",
      "loss:0.291850  [57664/100000]\n",
      "loss:0.267947  [64064/100000]\n",
      "loss:0.414344  [70464/100000]\n",
      "loss:0.326541  [76864/100000]\n",
      "loss:0.364148  [83264/100000]\n",
      "loss:0.316446  [89664/100000]\n",
      "loss:0.362033  [96064/100000]\n",
      "Test\n",
      " Avg Loss:0.956526  Accuracy:68.5\n",
      "Epoch28----------------\n",
      "\n",
      "loss:0.313759  [   64/100000]\n",
      "loss:0.432030  [ 6464/100000]\n",
      "loss:0.248735  [12864/100000]\n",
      "loss:0.327448  [19264/100000]\n",
      "loss:0.305786  [25664/100000]\n",
      "loss:0.747093  [32064/100000]\n",
      "loss:0.348416  [38464/100000]\n",
      "loss:0.337997  [44864/100000]\n",
      "loss:0.690140  [51264/100000]\n",
      "loss:0.305650  [57664/100000]\n",
      "loss:0.271818  [64064/100000]\n",
      "loss:0.439313  [70464/100000]\n",
      "loss:0.278765  [76864/100000]\n",
      "loss:0.389420  [83264/100000]\n",
      "loss:0.492220  [89664/100000]\n",
      "loss:0.448656  [96064/100000]\n",
      "Test\n",
      " Avg Loss:0.995552  Accuracy:68.1\n",
      "Epoch29----------------\n",
      "\n",
      "loss:0.259756  [   64/100000]\n",
      "loss:0.456202  [ 6464/100000]\n",
      "loss:0.278207  [12864/100000]\n",
      "loss:0.240018  [19264/100000]\n",
      "loss:0.253644  [25664/100000]\n",
      "loss:0.848306  [32064/100000]\n",
      "loss:0.321079  [38464/100000]\n",
      "loss:0.350210  [44864/100000]\n",
      "loss:0.630774  [51264/100000]\n",
      "loss:0.281820  [57664/100000]\n",
      "loss:0.221462  [64064/100000]\n",
      "loss:0.434241  [70464/100000]\n",
      "loss:0.299855  [76864/100000]\n",
      "loss:0.419421  [83264/100000]\n",
      "loss:0.477515  [89664/100000]\n",
      "loss:0.482792  [96064/100000]\n",
      "Test\n",
      " Avg Loss:1.046545  Accuracy:67.6\n",
      "Epoch30----------------\n",
      "\n",
      "loss:0.240932  [   64/100000]\n",
      "loss:0.389514  [ 6464/100000]\n",
      "loss:0.339715  [12864/100000]\n",
      "loss:0.280625  [19264/100000]\n",
      "loss:0.289548  [25664/100000]\n",
      "loss:0.898459  [32064/100000]\n",
      "loss:0.296957  [38464/100000]\n",
      "loss:0.313693  [44864/100000]\n",
      "loss:0.509316  [51264/100000]\n",
      "loss:0.310656  [57664/100000]\n",
      "loss:0.233342  [64064/100000]\n",
      "loss:0.413285  [70464/100000]\n",
      "loss:0.293652  [76864/100000]\n",
      "loss:0.348567  [83264/100000]\n",
      "loss:0.533940  [89664/100000]\n",
      "loss:0.388023  [96064/100000]\n",
      "Test\n",
      " Avg Loss:1.050754  Accuracy:67.4\n",
      "Epoch31----------------\n",
      "\n",
      "loss:0.240748  [   64/100000]\n",
      "loss:0.353288  [ 6464/100000]\n",
      "loss:0.282623  [12864/100000]\n",
      "loss:0.218119  [19264/100000]\n",
      "loss:0.246945  [25664/100000]\n",
      "loss:1.038032  [32064/100000]\n",
      "loss:0.284101  [38464/100000]\n",
      "loss:0.370234  [44864/100000]\n",
      "loss:0.543528  [51264/100000]\n",
      "loss:0.360714  [57664/100000]\n",
      "loss:0.242668  [64064/100000]\n",
      "loss:0.484860  [70464/100000]\n",
      "loss:0.293776  [76864/100000]\n",
      "loss:0.386058  [83264/100000]\n",
      "loss:0.392604  [89664/100000]\n",
      "loss:0.469628  [96064/100000]\n",
      "Test\n",
      " Avg Loss:1.086243  Accuracy:66.6\n",
      "Epoch32----------------\n",
      "\n",
      "loss:0.307965  [   64/100000]\n",
      "loss:0.412371  [ 6464/100000]\n",
      "loss:0.343753  [12864/100000]\n",
      "loss:0.250418  [19264/100000]\n",
      "loss:0.302930  [25664/100000]\n",
      "loss:0.671734  [32064/100000]\n",
      "loss:0.357067  [38464/100000]\n",
      "loss:0.315782  [44864/100000]\n",
      "loss:0.514841  [51264/100000]\n",
      "loss:0.321374  [57664/100000]\n",
      "loss:0.204815  [64064/100000]\n",
      "loss:0.408869  [70464/100000]\n",
      "loss:0.267504  [76864/100000]\n",
      "loss:0.361556  [83264/100000]\n",
      "loss:0.389039  [89664/100000]\n",
      "loss:0.360830  [96064/100000]\n",
      "Test\n",
      " Avg Loss:1.065493  Accuracy:67.8\n",
      "Epoch33----------------\n",
      "\n",
      "loss:0.267478  [   64/100000]\n",
      "loss:0.338214  [ 6464/100000]\n",
      "loss:0.294314  [12864/100000]\n",
      "loss:0.225114  [19264/100000]\n",
      "loss:0.220219  [25664/100000]\n",
      "loss:0.904275  [32064/100000]\n",
      "loss:0.304158  [38464/100000]\n",
      "loss:0.230871  [44864/100000]\n",
      "loss:0.451195  [51264/100000]\n",
      "loss:0.243979  [57664/100000]\n",
      "loss:0.284784  [64064/100000]\n",
      "loss:0.369311  [70464/100000]\n",
      "loss:0.255147  [76864/100000]\n",
      "loss:0.368522  [83264/100000]\n",
      "loss:0.329715  [89664/100000]\n",
      "loss:0.338288  [96064/100000]\n",
      "Test\n",
      " Avg Loss:1.116794  Accuracy:66.8\n",
      "Epoch34----------------\n",
      "\n",
      "loss:0.248588  [   64/100000]\n",
      "loss:0.392569  [ 6464/100000]\n",
      "loss:0.316319  [12864/100000]\n",
      "loss:0.245863  [19264/100000]\n",
      "loss:0.199162  [25664/100000]\n",
      "loss:0.807085  [32064/100000]\n",
      "loss:0.259103  [38464/100000]\n",
      "loss:0.230271  [44864/100000]\n",
      "loss:0.478155  [51264/100000]\n",
      "loss:0.354953  [57664/100000]\n",
      "loss:0.252631  [64064/100000]\n",
      "loss:0.459768  [70464/100000]\n",
      "loss:0.309449  [76864/100000]\n",
      "loss:0.323645  [83264/100000]\n",
      "loss:0.332794  [89664/100000]\n",
      "loss:0.387702  [96064/100000]\n",
      "Test\n",
      " Avg Loss:1.102060  Accuracy:67.0\n",
      "Epoch35----------------\n",
      "\n",
      "loss:0.311216  [   64/100000]\n",
      "loss:0.296770  [ 6464/100000]\n",
      "loss:0.228426  [12864/100000]\n",
      "loss:0.168337  [19264/100000]\n",
      "loss:0.175304  [25664/100000]\n",
      "loss:0.957849  [32064/100000]\n",
      "loss:0.251200  [38464/100000]\n",
      "loss:0.232393  [44864/100000]\n",
      "loss:0.487774  [51264/100000]\n",
      "loss:0.271862  [57664/100000]\n",
      "loss:0.261255  [64064/100000]\n",
      "loss:0.454638  [70464/100000]\n",
      "loss:0.270020  [76864/100000]\n",
      "loss:0.439102  [83264/100000]\n",
      "loss:0.403561  [89664/100000]\n",
      "loss:0.322038  [96064/100000]\n",
      "Test\n",
      " Avg Loss:1.169400  Accuracy:66.6\n",
      "Epoch36----------------\n",
      "\n",
      "loss:0.221438  [   64/100000]\n",
      "loss:0.344354  [ 6464/100000]\n",
      "loss:0.228430  [12864/100000]\n",
      "loss:0.179219  [19264/100000]\n",
      "loss:0.214366  [25664/100000]\n",
      "loss:0.817787  [32064/100000]\n",
      "loss:0.352078  [38464/100000]\n",
      "loss:0.273033  [44864/100000]\n",
      "loss:0.413953  [51264/100000]\n",
      "loss:0.295010  [57664/100000]\n",
      "loss:0.283496  [64064/100000]\n",
      "loss:0.463340  [70464/100000]\n",
      "loss:0.323364  [76864/100000]\n",
      "loss:0.350311  [83264/100000]\n",
      "loss:0.373847  [89664/100000]\n",
      "loss:0.388539  [96064/100000]\n",
      "Test\n",
      " Avg Loss:1.130230  Accuracy:67.8\n",
      "Epoch37----------------\n",
      "\n",
      "loss:0.277571  [   64/100000]\n",
      "loss:0.391557  [ 6464/100000]\n",
      "loss:0.235748  [12864/100000]\n",
      "loss:0.280312  [19264/100000]\n",
      "loss:0.240372  [25664/100000]\n",
      "loss:0.870158  [32064/100000]\n",
      "loss:0.263968  [38464/100000]\n",
      "loss:0.253571  [44864/100000]\n",
      "loss:0.405554  [51264/100000]\n",
      "loss:0.340708  [57664/100000]\n",
      "loss:0.238700  [64064/100000]\n",
      "loss:0.377940  [70464/100000]\n",
      "loss:0.282893  [76864/100000]\n",
      "loss:0.321226  [83264/100000]\n",
      "loss:0.292941  [89664/100000]\n",
      "loss:0.284205  [96064/100000]\n",
      "Test\n",
      " Avg Loss:1.206683  Accuracy:67.1\n",
      "Epoch38----------------\n",
      "\n",
      "loss:0.267756  [   64/100000]\n",
      "loss:0.282858  [ 6464/100000]\n",
      "loss:0.309498  [12864/100000]\n",
      "loss:0.174361  [19264/100000]\n",
      "loss:0.200341  [25664/100000]\n",
      "loss:0.722842  [32064/100000]\n",
      "loss:0.434349  [38464/100000]\n",
      "loss:0.245931  [44864/100000]\n",
      "loss:0.506441  [51264/100000]\n",
      "loss:0.312235  [57664/100000]\n",
      "loss:0.299039  [64064/100000]\n",
      "loss:0.429837  [70464/100000]\n",
      "loss:0.251427  [76864/100000]\n",
      "loss:0.394446  [83264/100000]\n",
      "loss:0.330857  [89664/100000]\n",
      "loss:0.332097  [96064/100000]\n",
      "Test\n",
      " Avg Loss:1.164964  Accuracy:68.1\n",
      "Epoch39----------------\n",
      "\n",
      "loss:0.258899  [   64/100000]\n",
      "loss:0.302063  [ 6464/100000]\n",
      "loss:0.262819  [12864/100000]\n",
      "loss:0.189930  [19264/100000]\n",
      "loss:0.257480  [25664/100000]\n",
      "loss:0.751484  [32064/100000]\n",
      "loss:0.294871  [38464/100000]\n",
      "loss:0.200920  [44864/100000]\n",
      "loss:0.455786  [51264/100000]\n",
      "loss:0.224433  [57664/100000]\n",
      "loss:0.311523  [64064/100000]\n",
      "loss:0.487257  [70464/100000]\n",
      "loss:0.255435  [76864/100000]\n",
      "loss:0.322631  [83264/100000]\n",
      "loss:0.341622  [89664/100000]\n",
      "loss:0.292043  [96064/100000]\n",
      "Test\n",
      " Avg Loss:1.159647  Accuracy:67.5\n",
      "Epoch40----------------\n",
      "\n",
      "loss:0.275233  [   64/100000]\n",
      "loss:0.282299  [ 6464/100000]\n",
      "loss:0.269862  [12864/100000]\n",
      "loss:0.135909  [19264/100000]\n",
      "loss:0.146690  [25664/100000]\n",
      "loss:0.582111  [32064/100000]\n",
      "loss:0.325485  [38464/100000]\n",
      "loss:0.232994  [44864/100000]\n",
      "loss:0.444488  [51264/100000]\n",
      "loss:0.292439  [57664/100000]\n",
      "loss:0.234192  [64064/100000]\n",
      "loss:0.356381  [70464/100000]\n",
      "loss:0.263271  [76864/100000]\n",
      "loss:0.318457  [83264/100000]\n",
      "loss:0.273504  [89664/100000]\n",
      "loss:0.321687  [96064/100000]\n",
      "Test\n",
      " Avg Loss:1.222904  Accuracy:67.1\n",
      "Epoch41----------------\n",
      "\n",
      "loss:0.252685  [   64/100000]\n",
      "loss:0.318075  [ 6464/100000]\n",
      "loss:0.316148  [12864/100000]\n",
      "loss:0.221713  [19264/100000]\n",
      "loss:0.150355  [25664/100000]\n",
      "loss:0.746970  [32064/100000]\n",
      "loss:0.343014  [38464/100000]\n",
      "loss:0.287600  [44864/100000]\n",
      "loss:0.410479  [51264/100000]\n",
      "loss:0.307956  [57664/100000]\n",
      "loss:0.201555  [64064/100000]\n",
      "loss:0.433790  [70464/100000]\n",
      "loss:0.281818  [76864/100000]\n",
      "loss:0.399892  [83264/100000]\n",
      "loss:0.358751  [89664/100000]\n",
      "loss:0.317934  [96064/100000]\n",
      "Test\n",
      " Avg Loss:1.210261  Accuracy:67.7\n",
      "Epoch42----------------\n",
      "\n",
      "loss:0.231551  [   64/100000]\n",
      "loss:0.381728  [ 6464/100000]\n",
      "loss:0.358478  [12864/100000]\n",
      "loss:0.170071  [19264/100000]\n",
      "loss:0.193164  [25664/100000]\n",
      "loss:0.728308  [32064/100000]\n",
      "loss:0.290772  [38464/100000]\n",
      "loss:0.228544  [44864/100000]\n",
      "loss:0.525006  [51264/100000]\n",
      "loss:0.213784  [57664/100000]\n",
      "loss:0.186646  [64064/100000]\n",
      "loss:0.377396  [70464/100000]\n",
      "loss:0.268234  [76864/100000]\n",
      "loss:0.311012  [83264/100000]\n",
      "loss:0.257447  [89664/100000]\n",
      "loss:0.306978  [96064/100000]\n",
      "Test\n",
      " Avg Loss:1.269760  Accuracy:66.6\n",
      "Epoch43----------------\n",
      "\n",
      "loss:0.248407  [   64/100000]\n",
      "loss:0.268039  [ 6464/100000]\n",
      "loss:0.348510  [12864/100000]\n",
      "loss:0.143591  [19264/100000]\n",
      "loss:0.149520  [25664/100000]\n",
      "loss:0.590214  [32064/100000]\n",
      "loss:0.246432  [38464/100000]\n",
      "loss:0.272171  [44864/100000]\n",
      "loss:0.484838  [51264/100000]\n",
      "loss:0.225535  [57664/100000]\n",
      "loss:0.274520  [64064/100000]\n",
      "loss:0.434779  [70464/100000]\n",
      "loss:0.286306  [76864/100000]\n",
      "loss:0.357168  [83264/100000]\n",
      "loss:0.318580  [89664/100000]\n",
      "loss:0.297625  [96064/100000]\n",
      "Test\n",
      " Avg Loss:1.303654  Accuracy:66.2\n",
      "Epoch44----------------\n",
      "\n",
      "loss:0.242420  [   64/100000]\n",
      "loss:0.260714  [ 6464/100000]\n",
      "loss:0.249179  [12864/100000]\n",
      "loss:0.169705  [19264/100000]\n",
      "loss:0.236392  [25664/100000]\n",
      "loss:0.725543  [32064/100000]\n",
      "loss:0.277062  [38464/100000]\n",
      "loss:0.305420  [44864/100000]\n",
      "loss:0.503024  [51264/100000]\n",
      "loss:0.247230  [57664/100000]\n",
      "loss:0.215482  [64064/100000]\n",
      "loss:0.325260  [70464/100000]\n",
      "loss:0.234670  [76864/100000]\n",
      "loss:0.327859  [83264/100000]\n",
      "loss:0.354656  [89664/100000]\n",
      "loss:0.257731  [96064/100000]\n",
      "Test\n",
      " Avg Loss:1.283247  Accuracy:67.5\n",
      "Epoch45----------------\n",
      "\n",
      "loss:0.318771  [   64/100000]\n",
      "loss:0.333309  [ 6464/100000]\n",
      "loss:0.285026  [12864/100000]\n",
      "loss:0.174107  [19264/100000]\n",
      "loss:0.168366  [25664/100000]\n",
      "loss:0.570686  [32064/100000]\n",
      "loss:0.306102  [38464/100000]\n",
      "loss:0.291723  [44864/100000]\n",
      "loss:0.464124  [51264/100000]\n",
      "loss:0.265016  [57664/100000]\n",
      "loss:0.285182  [64064/100000]\n",
      "loss:0.435059  [70464/100000]\n",
      "loss:0.211216  [76864/100000]\n",
      "loss:0.335226  [83264/100000]\n",
      "loss:0.398357  [89664/100000]\n",
      "loss:0.295819  [96064/100000]\n",
      "Test\n",
      " Avg Loss:1.250170  Accuracy:67.4\n",
      "Epoch46----------------\n",
      "\n",
      "loss:0.317035  [   64/100000]\n",
      "loss:0.343708  [ 6464/100000]\n",
      "loss:0.360258  [12864/100000]\n",
      "loss:0.194254  [19264/100000]\n",
      "loss:0.166287  [25664/100000]\n",
      "loss:0.591536  [32064/100000]\n",
      "loss:0.463411  [38464/100000]\n",
      "loss:0.176018  [44864/100000]\n",
      "loss:0.436920  [51264/100000]\n",
      "loss:0.250703  [57664/100000]\n",
      "loss:0.186626  [64064/100000]\n",
      "loss:0.401310  [70464/100000]\n",
      "loss:0.272058  [76864/100000]\n",
      "loss:0.273349  [83264/100000]\n",
      "loss:0.243182  [89664/100000]\n",
      "loss:0.284568  [96064/100000]\n",
      "Test\n",
      " Avg Loss:1.293636  Accuracy:67.7\n",
      "Epoch47----------------\n",
      "\n",
      "loss:0.232730  [   64/100000]\n",
      "loss:0.312099  [ 6464/100000]\n",
      "loss:0.277304  [12864/100000]\n",
      "loss:0.186074  [19264/100000]\n",
      "loss:0.211395  [25664/100000]\n",
      "loss:0.617267  [32064/100000]\n",
      "loss:0.255472  [38464/100000]\n",
      "loss:0.279216  [44864/100000]\n",
      "loss:0.375921  [51264/100000]\n",
      "loss:0.259507  [57664/100000]\n",
      "loss:0.228268  [64064/100000]\n",
      "loss:0.309056  [70464/100000]\n",
      "loss:0.321070  [76864/100000]\n",
      "loss:0.481563  [83264/100000]\n",
      "loss:0.363725  [89664/100000]\n",
      "loss:0.342710  [96064/100000]\n",
      "Test\n",
      " Avg Loss:1.322827  Accuracy:66.8\n",
      "Epoch48----------------\n",
      "\n",
      "loss:0.166427  [   64/100000]\n",
      "loss:0.276146  [ 6464/100000]\n",
      "loss:0.224349  [12864/100000]\n",
      "loss:0.251089  [19264/100000]\n",
      "loss:0.170259  [25664/100000]\n",
      "loss:0.611897  [32064/100000]\n",
      "loss:0.276586  [38464/100000]\n",
      "loss:0.281823  [44864/100000]\n",
      "loss:0.434641  [51264/100000]\n",
      "loss:0.243782  [57664/100000]\n",
      "loss:0.215002  [64064/100000]\n",
      "loss:0.403340  [70464/100000]\n",
      "loss:0.298276  [76864/100000]\n",
      "loss:0.385933  [83264/100000]\n",
      "loss:0.303749  [89664/100000]\n",
      "loss:0.279552  [96064/100000]\n",
      "Test\n",
      " Avg Loss:1.330872  Accuracy:66.5\n",
      "Epoch49----------------\n",
      "\n",
      "loss:0.286822  [   64/100000]\n",
      "loss:0.296400  [ 6464/100000]\n",
      "loss:0.238208  [12864/100000]\n",
      "loss:0.145540  [19264/100000]\n",
      "loss:0.156893  [25664/100000]\n",
      "loss:0.611288  [32064/100000]\n",
      "loss:0.361908  [38464/100000]\n",
      "loss:0.303138  [44864/100000]\n",
      "loss:0.407397  [51264/100000]\n",
      "loss:0.287286  [57664/100000]\n",
      "loss:0.126220  [64064/100000]\n",
      "loss:0.333273  [70464/100000]\n",
      "loss:0.414834  [76864/100000]\n",
      "loss:0.372832  [83264/100000]\n",
      "loss:0.373974  [89664/100000]\n",
      "loss:0.315778  [96064/100000]\n",
      "Test\n",
      " Avg Loss:1.385740  Accuracy:66.7\n",
      "Epoch50----------------\n",
      "\n",
      "loss:0.224834  [   64/100000]\n",
      "loss:0.289337  [ 6464/100000]\n",
      "loss:0.280309  [12864/100000]\n",
      "loss:0.159893  [19264/100000]\n",
      "loss:0.201538  [25664/100000]\n",
      "loss:0.665730  [32064/100000]\n",
      "loss:0.276313  [38464/100000]\n",
      "loss:0.366215  [44864/100000]\n",
      "loss:0.495820  [51264/100000]\n",
      "loss:0.238624  [57664/100000]\n",
      "loss:0.140599  [64064/100000]\n",
      "loss:0.344415  [70464/100000]\n",
      "loss:0.333701  [76864/100000]\n",
      "loss:0.340176  [83264/100000]\n",
      "loss:0.357425  [89664/100000]\n",
      "loss:0.300242  [96064/100000]\n",
      "Test\n",
      " Avg Loss:1.422590  Accuracy:66.2\n",
      "Epoch51----------------\n",
      "\n",
      "loss:0.214971  [   64/100000]\n",
      "loss:0.261222  [ 6464/100000]\n",
      "loss:0.402321  [12864/100000]\n",
      "loss:0.155268  [19264/100000]\n",
      "loss:0.200490  [25664/100000]\n",
      "loss:0.634124  [32064/100000]\n",
      "loss:0.336805  [38464/100000]\n",
      "loss:0.216599  [44864/100000]\n",
      "loss:0.337013  [51264/100000]\n",
      "loss:0.232036  [57664/100000]\n",
      "loss:0.157056  [64064/100000]\n",
      "loss:0.386903  [70464/100000]\n",
      "loss:0.293367  [76864/100000]\n",
      "loss:0.282727  [83264/100000]\n",
      "loss:0.345090  [89664/100000]\n",
      "loss:0.287244  [96064/100000]\n",
      "Test\n",
      " Avg Loss:1.440325  Accuracy:66.1\n",
      "Epoch52----------------\n",
      "\n",
      "loss:0.264064  [   64/100000]\n",
      "loss:0.238457  [ 6464/100000]\n",
      "loss:0.382045  [12864/100000]\n",
      "loss:0.146932  [19264/100000]\n",
      "loss:0.115506  [25664/100000]\n",
      "loss:0.598354  [32064/100000]\n",
      "loss:0.240190  [38464/100000]\n",
      "loss:0.315311  [44864/100000]\n",
      "loss:0.428390  [51264/100000]\n",
      "loss:0.302062  [57664/100000]\n",
      "loss:0.164049  [64064/100000]\n",
      "loss:0.328855  [70464/100000]\n",
      "loss:0.290477  [76864/100000]\n",
      "loss:0.334394  [83264/100000]\n",
      "loss:0.248591  [89664/100000]\n",
      "loss:0.359162  [96064/100000]\n",
      "Test\n",
      " Avg Loss:1.477962  Accuracy:65.9\n",
      "Epoch53----------------\n",
      "\n",
      "loss:0.236192  [   64/100000]\n",
      "loss:0.351375  [ 6464/100000]\n",
      "loss:0.295996  [12864/100000]\n",
      "loss:0.194539  [19264/100000]\n",
      "loss:0.162925  [25664/100000]\n",
      "loss:0.580204  [32064/100000]\n",
      "loss:0.233697  [38464/100000]\n",
      "loss:0.208169  [44864/100000]\n",
      "loss:0.400286  [51264/100000]\n",
      "loss:0.197849  [57664/100000]\n",
      "loss:0.133189  [64064/100000]\n",
      "loss:0.302929  [70464/100000]\n",
      "loss:0.268222  [76864/100000]\n",
      "loss:0.360364  [83264/100000]\n",
      "loss:0.219104  [89664/100000]\n",
      "loss:0.233765  [96064/100000]\n",
      "Test\n",
      " Avg Loss:1.497156  Accuracy:65.9\n",
      "Epoch54----------------\n",
      "\n",
      "loss:0.201981  [   64/100000]\n",
      "loss:0.265837  [ 6464/100000]\n",
      "loss:0.305444  [12864/100000]\n",
      "loss:0.180012  [19264/100000]\n",
      "loss:0.144395  [25664/100000]\n",
      "loss:0.533922  [32064/100000]\n",
      "loss:0.246167  [38464/100000]\n",
      "loss:0.263222  [44864/100000]\n",
      "loss:0.446898  [51264/100000]\n",
      "loss:0.231426  [57664/100000]\n",
      "loss:0.212434  [64064/100000]\n",
      "loss:0.400975  [70464/100000]\n",
      "loss:0.295262  [76864/100000]\n",
      "loss:0.312167  [83264/100000]\n",
      "loss:0.255797  [89664/100000]\n",
      "loss:0.298630  [96064/100000]\n",
      "Test\n",
      " Avg Loss:1.464403  Accuracy:66.6\n",
      "Epoch55----------------\n",
      "\n",
      "loss:0.236544  [   64/100000]\n",
      "loss:0.302421  [ 6464/100000]\n",
      "loss:0.205244  [12864/100000]\n",
      "loss:0.142288  [19264/100000]\n",
      "loss:0.160920  [25664/100000]\n",
      "loss:0.585685  [32064/100000]\n",
      "loss:0.280455  [38464/100000]\n",
      "loss:0.282344  [44864/100000]\n",
      "loss:0.453544  [51264/100000]\n",
      "loss:0.367453  [57664/100000]\n",
      "loss:0.192779  [64064/100000]\n",
      "loss:0.269755  [70464/100000]\n",
      "loss:0.267065  [76864/100000]\n",
      "loss:0.345298  [83264/100000]\n",
      "loss:0.381335  [89664/100000]\n",
      "loss:0.321893  [96064/100000]\n",
      "Test\n",
      " Avg Loss:1.521253  Accuracy:66.3\n",
      "Epoch56----------------\n",
      "\n",
      "loss:0.145268  [   64/100000]\n",
      "loss:0.228075  [ 6464/100000]\n",
      "loss:0.365929  [12864/100000]\n",
      "loss:0.171004  [19264/100000]\n",
      "loss:0.209355  [25664/100000]\n",
      "loss:0.604757  [32064/100000]\n",
      "loss:0.260835  [38464/100000]\n",
      "loss:0.210077  [44864/100000]\n",
      "loss:0.512874  [51264/100000]\n",
      "loss:0.353210  [57664/100000]\n",
      "loss:0.144497  [64064/100000]\n",
      "loss:0.300534  [70464/100000]\n",
      "loss:0.172687  [76864/100000]\n",
      "loss:0.369213  [83264/100000]\n",
      "loss:0.185504  [89664/100000]\n",
      "loss:0.295088  [96064/100000]\n",
      "Test\n",
      " Avg Loss:1.504905  Accuracy:65.9\n",
      "Epoch57----------------\n",
      "\n",
      "loss:0.173538  [   64/100000]\n",
      "loss:0.183491  [ 6464/100000]\n",
      "loss:0.201249  [12864/100000]\n",
      "loss:0.127150  [19264/100000]\n",
      "loss:0.174506  [25664/100000]\n",
      "loss:0.494562  [32064/100000]\n",
      "loss:0.258844  [38464/100000]\n",
      "loss:0.186259  [44864/100000]\n",
      "loss:0.316857  [51264/100000]\n",
      "loss:0.251417  [57664/100000]\n",
      "loss:0.190670  [64064/100000]\n",
      "loss:0.246031  [70464/100000]\n",
      "loss:0.228291  [76864/100000]\n",
      "loss:0.370863  [83264/100000]\n",
      "loss:0.209445  [89664/100000]\n",
      "loss:0.285931  [96064/100000]\n",
      "Test\n",
      " Avg Loss:1.537171  Accuracy:66.5\n",
      "Epoch58----------------\n",
      "\n",
      "loss:0.225445  [   64/100000]\n",
      "loss:0.191427  [ 6464/100000]\n",
      "loss:0.321181  [12864/100000]\n",
      "loss:0.175609  [19264/100000]\n",
      "loss:0.137551  [25664/100000]\n",
      "loss:0.480038  [32064/100000]\n",
      "loss:0.234241  [38464/100000]\n",
      "loss:0.171985  [44864/100000]\n",
      "loss:0.390940  [51264/100000]\n",
      "loss:0.254204  [57664/100000]\n",
      "loss:0.151149  [64064/100000]\n",
      "loss:0.292343  [70464/100000]\n",
      "loss:0.301840  [76864/100000]\n",
      "loss:0.304386  [83264/100000]\n",
      "loss:0.221125  [89664/100000]\n",
      "loss:0.340229  [96064/100000]\n",
      "Test\n",
      " Avg Loss:1.559699  Accuracy:66.0\n",
      "Epoch59----------------\n",
      "\n",
      "loss:0.147126  [   64/100000]\n",
      "loss:0.209519  [ 6464/100000]\n",
      "loss:0.261554  [12864/100000]\n",
      "loss:0.128130  [19264/100000]\n",
      "loss:0.107813  [25664/100000]\n",
      "loss:0.507263  [32064/100000]\n",
      "loss:0.357004  [38464/100000]\n",
      "loss:0.198521  [44864/100000]\n",
      "loss:0.537889  [51264/100000]\n",
      "loss:0.257695  [57664/100000]\n",
      "loss:0.148732  [64064/100000]\n",
      "loss:0.260633  [70464/100000]\n",
      "loss:0.189954  [76864/100000]\n",
      "loss:0.420945  [83264/100000]\n",
      "loss:0.245394  [89664/100000]\n",
      "loss:0.267286  [96064/100000]\n",
      "Test\n",
      " Avg Loss:1.599717  Accuracy:66.1\n",
      "Epoch60----------------\n",
      "\n",
      "loss:0.204228  [   64/100000]\n",
      "loss:0.454208  [ 6464/100000]\n",
      "loss:0.267417  [12864/100000]\n",
      "loss:0.195463  [19264/100000]\n",
      "loss:0.134435  [25664/100000]\n",
      "loss:0.443215  [32064/100000]\n",
      "loss:0.214502  [38464/100000]\n",
      "loss:0.275410  [44864/100000]\n",
      "loss:0.455295  [51264/100000]\n",
      "loss:0.270604  [57664/100000]\n",
      "loss:0.325942  [64064/100000]\n",
      "loss:0.209701  [70464/100000]\n",
      "loss:0.186473  [76864/100000]\n",
      "loss:0.436438  [83264/100000]\n",
      "loss:0.198493  [89664/100000]\n",
      "loss:0.400327  [96064/100000]\n",
      "Test\n",
      " Avg Loss:1.599137  Accuracy:65.5\n",
      "Epoch61----------------\n",
      "\n",
      "loss:0.184061  [   64/100000]\n",
      "loss:0.195124  [ 6464/100000]\n",
      "loss:0.316725  [12864/100000]\n",
      "loss:0.256012  [19264/100000]\n",
      "loss:0.155682  [25664/100000]\n",
      "loss:0.537655  [32064/100000]\n",
      "loss:0.241747  [38464/100000]\n",
      "loss:0.276429  [44864/100000]\n",
      "loss:0.331912  [51264/100000]\n",
      "loss:0.258088  [57664/100000]\n",
      "loss:0.147677  [64064/100000]\n",
      "loss:0.338536  [70464/100000]\n",
      "loss:0.151846  [76864/100000]\n",
      "loss:0.365429  [83264/100000]\n",
      "loss:0.175801  [89664/100000]\n",
      "loss:0.288394  [96064/100000]\n",
      "Test\n",
      " Avg Loss:1.621602  Accuracy:65.1\n",
      "Epoch62----------------\n",
      "\n",
      "loss:0.312288  [   64/100000]\n",
      "loss:0.302663  [ 6464/100000]\n",
      "loss:0.196016  [12864/100000]\n",
      "loss:0.179718  [19264/100000]\n",
      "loss:0.236570  [25664/100000]\n",
      "loss:0.444355  [32064/100000]\n",
      "loss:0.180524  [38464/100000]\n",
      "loss:0.211637  [44864/100000]\n",
      "loss:0.339234  [51264/100000]\n",
      "loss:0.211437  [57664/100000]\n",
      "loss:0.333416  [64064/100000]\n",
      "loss:0.299964  [70464/100000]\n",
      "loss:0.336812  [76864/100000]\n",
      "loss:0.376253  [83264/100000]\n",
      "loss:0.158011  [89664/100000]\n",
      "loss:0.351988  [96064/100000]\n",
      "Test\n",
      " Avg Loss:1.595758  Accuracy:66.7\n",
      "Epoch63----------------\n",
      "\n",
      "loss:0.195108  [   64/100000]\n",
      "loss:0.315710  [ 6464/100000]\n",
      "loss:0.237104  [12864/100000]\n",
      "loss:0.162530  [19264/100000]\n",
      "loss:0.174084  [25664/100000]\n",
      "loss:0.480024  [32064/100000]\n",
      "loss:0.209233  [38464/100000]\n",
      "loss:0.197087  [44864/100000]\n",
      "loss:0.375859  [51264/100000]\n",
      "loss:0.231580  [57664/100000]\n",
      "loss:0.138119  [64064/100000]\n",
      "loss:0.281453  [70464/100000]\n",
      "loss:0.160019  [76864/100000]\n",
      "loss:0.346069  [83264/100000]\n",
      "loss:0.194281  [89664/100000]\n",
      "loss:0.316077  [96064/100000]\n",
      "Test\n",
      " Avg Loss:1.627627  Accuracy:66.1\n",
      "Epoch64----------------\n",
      "\n",
      "loss:0.188958  [   64/100000]\n",
      "loss:0.290656  [ 6464/100000]\n",
      "loss:0.295884  [12864/100000]\n",
      "loss:0.183220  [19264/100000]\n",
      "loss:0.153042  [25664/100000]\n",
      "loss:0.454178  [32064/100000]\n",
      "loss:0.180259  [38464/100000]\n",
      "loss:0.151689  [44864/100000]\n",
      "loss:0.281431  [51264/100000]\n",
      "loss:0.186329  [57664/100000]\n",
      "loss:0.143282  [64064/100000]\n",
      "loss:0.244227  [70464/100000]\n",
      "loss:0.244702  [76864/100000]\n",
      "loss:0.299910  [83264/100000]\n",
      "loss:0.192334  [89664/100000]\n",
      "loss:0.312617  [96064/100000]\n",
      "Test\n",
      " Avg Loss:1.666275  Accuracy:65.7\n",
      "Epoch65----------------\n",
      "\n",
      "loss:0.222679  [   64/100000]\n",
      "loss:0.268016  [ 6464/100000]\n",
      "loss:0.210945  [12864/100000]\n",
      "loss:0.211059  [19264/100000]\n",
      "loss:0.234368  [25664/100000]\n",
      "loss:0.419957  [32064/100000]\n",
      "loss:0.269833  [38464/100000]\n",
      "loss:0.310320  [44864/100000]\n",
      "loss:0.391158  [51264/100000]\n",
      "loss:0.184871  [57664/100000]\n",
      "loss:0.171246  [64064/100000]\n",
      "loss:0.364976  [70464/100000]\n",
      "loss:0.330018  [76864/100000]\n",
      "loss:0.251310  [83264/100000]\n",
      "loss:0.227245  [89664/100000]\n",
      "loss:0.300076  [96064/100000]\n",
      "Test\n",
      " Avg Loss:1.622644  Accuracy:66.2\n",
      "Epoch66----------------\n",
      "\n",
      "loss:0.190529  [   64/100000]\n",
      "loss:0.172881  [ 6464/100000]\n",
      "loss:0.226452  [12864/100000]\n",
      "loss:0.169977  [19264/100000]\n",
      "loss:0.146599  [25664/100000]\n",
      "loss:0.472833  [32064/100000]\n",
      "loss:0.210650  [38464/100000]\n",
      "loss:0.250750  [44864/100000]\n",
      "loss:0.550267  [51264/100000]\n",
      "loss:0.201834  [57664/100000]\n",
      "loss:0.158173  [64064/100000]\n",
      "loss:0.400645  [70464/100000]\n",
      "loss:0.222813  [76864/100000]\n",
      "loss:0.279930  [83264/100000]\n",
      "loss:0.291093  [89664/100000]\n",
      "loss:0.247186  [96064/100000]\n",
      "Test\n",
      " Avg Loss:1.623312  Accuracy:66.4\n",
      "Epoch67----------------\n",
      "\n",
      "loss:0.186859  [   64/100000]\n",
      "loss:0.254571  [ 6464/100000]\n",
      "loss:0.306481  [12864/100000]\n",
      "loss:0.166346  [19264/100000]\n",
      "loss:0.162047  [25664/100000]\n",
      "loss:0.517314  [32064/100000]\n",
      "loss:0.215519  [38464/100000]\n",
      "loss:0.161424  [44864/100000]\n",
      "loss:0.311965  [51264/100000]\n",
      "loss:0.242586  [57664/100000]\n",
      "loss:0.229670  [64064/100000]\n",
      "loss:0.291877  [70464/100000]\n",
      "loss:0.282175  [76864/100000]\n",
      "loss:0.295197  [83264/100000]\n",
      "loss:0.212474  [89664/100000]\n",
      "loss:0.204637  [96064/100000]\n",
      "Test\n",
      " Avg Loss:1.715319  Accuracy:66.0\n",
      "Epoch68----------------\n",
      "\n",
      "loss:0.148215  [   64/100000]\n",
      "loss:0.255183  [ 6464/100000]\n",
      "loss:0.218443  [12864/100000]\n",
      "loss:0.232912  [19264/100000]\n",
      "loss:0.126834  [25664/100000]\n",
      "loss:0.418379  [32064/100000]\n",
      "loss:0.126675  [38464/100000]\n",
      "loss:0.119118  [44864/100000]\n",
      "loss:0.643023  [51264/100000]\n",
      "loss:0.245338  [57664/100000]\n",
      "loss:0.149951  [64064/100000]\n",
      "loss:0.242278  [70464/100000]\n",
      "loss:0.213157  [76864/100000]\n",
      "loss:0.348625  [83264/100000]\n",
      "loss:0.257912  [89664/100000]\n",
      "loss:0.251347  [96064/100000]\n",
      "Test\n",
      " Avg Loss:1.727065  Accuracy:65.0\n",
      "Epoch69----------------\n",
      "\n",
      "loss:0.176399  [   64/100000]\n",
      "loss:0.291208  [ 6464/100000]\n",
      "loss:0.253794  [12864/100000]\n",
      "loss:0.146735  [19264/100000]\n",
      "loss:0.157736  [25664/100000]\n",
      "loss:0.383502  [32064/100000]\n",
      "loss:0.218872  [38464/100000]\n",
      "loss:0.170447  [44864/100000]\n",
      "loss:0.480484  [51264/100000]\n",
      "loss:0.294353  [57664/100000]\n",
      "loss:0.125392  [64064/100000]\n",
      "loss:0.345457  [70464/100000]\n",
      "loss:0.249870  [76864/100000]\n",
      "loss:0.292544  [83264/100000]\n",
      "loss:0.207566  [89664/100000]\n",
      "loss:0.205668  [96064/100000]\n",
      "Test\n",
      " Avg Loss:1.704859  Accuracy:66.3\n",
      "Epoch70----------------\n",
      "\n",
      "loss:0.222278  [   64/100000]\n",
      "loss:0.171166  [ 6464/100000]\n",
      "loss:0.287055  [12864/100000]\n",
      "loss:0.141950  [19264/100000]\n",
      "loss:0.176637  [25664/100000]\n",
      "loss:0.433944  [32064/100000]\n",
      "loss:0.101817  [38464/100000]\n",
      "loss:0.228163  [44864/100000]\n",
      "loss:0.402070  [51264/100000]\n",
      "loss:0.235603  [57664/100000]\n",
      "loss:0.246374  [64064/100000]\n",
      "loss:0.331389  [70464/100000]\n",
      "loss:0.232306  [76864/100000]\n",
      "loss:0.297456  [83264/100000]\n",
      "loss:0.242373  [89664/100000]\n",
      "loss:0.211177  [96064/100000]\n",
      "Test\n",
      " Avg Loss:1.813253  Accuracy:65.4\n",
      "Epoch71----------------\n",
      "\n",
      "loss:0.147785  [   64/100000]\n",
      "loss:0.236816  [ 6464/100000]\n",
      "loss:0.184588  [12864/100000]\n",
      "loss:0.179379  [19264/100000]\n",
      "loss:0.187811  [25664/100000]\n",
      "loss:0.370018  [32064/100000]\n",
      "loss:0.156813  [38464/100000]\n",
      "loss:0.094436  [44864/100000]\n",
      "loss:0.435301  [51264/100000]\n",
      "loss:0.219839  [57664/100000]\n",
      "loss:0.284824  [64064/100000]\n",
      "loss:0.196677  [70464/100000]\n",
      "loss:0.178228  [76864/100000]\n",
      "loss:0.283038  [83264/100000]\n",
      "loss:0.168233  [89664/100000]\n",
      "loss:0.388031  [96064/100000]\n",
      "Test\n",
      " Avg Loss:1.722500  Accuracy:66.6\n",
      "Epoch72----------------\n",
      "\n",
      "loss:0.211090  [   64/100000]\n",
      "loss:0.247567  [ 6464/100000]\n",
      "loss:0.172085  [12864/100000]\n",
      "loss:0.163450  [19264/100000]\n",
      "loss:0.174795  [25664/100000]\n",
      "loss:0.337532  [32064/100000]\n",
      "loss:0.163019  [38464/100000]\n",
      "loss:0.129921  [44864/100000]\n",
      "loss:0.334366  [51264/100000]\n",
      "loss:0.218504  [57664/100000]\n",
      "loss:0.295571  [64064/100000]\n",
      "loss:0.335918  [70464/100000]\n",
      "loss:0.179328  [76864/100000]\n",
      "loss:0.309911  [83264/100000]\n",
      "loss:0.171604  [89664/100000]\n",
      "loss:0.246627  [96064/100000]\n",
      "Test\n",
      " Avg Loss:1.804884  Accuracy:65.6\n",
      "Epoch73----------------\n",
      "\n",
      "loss:0.234262  [   64/100000]\n",
      "loss:0.153094  [ 6464/100000]\n",
      "loss:0.182335  [12864/100000]\n",
      "loss:0.178389  [19264/100000]\n",
      "loss:0.202787  [25664/100000]\n",
      "loss:0.297520  [32064/100000]\n",
      "loss:0.181173  [38464/100000]\n",
      "loss:0.209936  [44864/100000]\n",
      "loss:0.434590  [51264/100000]\n",
      "loss:0.243986  [57664/100000]\n",
      "loss:0.233936  [64064/100000]\n",
      "loss:0.240580  [70464/100000]\n",
      "loss:0.208583  [76864/100000]\n",
      "loss:0.390428  [83264/100000]\n",
      "loss:0.213654  [89664/100000]\n",
      "loss:0.284506  [96064/100000]\n",
      "Test\n",
      " Avg Loss:1.801800  Accuracy:66.1\n",
      "Epoch74----------------\n",
      "\n",
      "loss:0.268944  [   64/100000]\n",
      "loss:0.205358  [ 6464/100000]\n",
      "loss:0.232093  [12864/100000]\n",
      "loss:0.175499  [19264/100000]\n",
      "loss:0.119512  [25664/100000]\n",
      "loss:0.555702  [32064/100000]\n",
      "loss:0.222077  [38464/100000]\n",
      "loss:0.213681  [44864/100000]\n",
      "loss:0.476210  [51264/100000]\n",
      "loss:0.202838  [57664/100000]\n",
      "loss:0.130389  [64064/100000]\n",
      "loss:0.341626  [70464/100000]\n",
      "loss:0.278052  [76864/100000]\n",
      "loss:0.273568  [83264/100000]\n",
      "loss:0.249067  [89664/100000]\n",
      "loss:0.230529  [96064/100000]\n",
      "Test\n",
      " Avg Loss:1.766563  Accuracy:66.2\n",
      "Epoch75----------------\n",
      "\n",
      "loss:0.182458  [   64/100000]\n",
      "loss:0.236439  [ 6464/100000]\n",
      "loss:0.295230  [12864/100000]\n",
      "loss:0.153087  [19264/100000]\n",
      "loss:0.170799  [25664/100000]\n",
      "loss:0.395496  [32064/100000]\n",
      "loss:0.144348  [38464/100000]\n",
      "loss:0.192303  [44864/100000]\n",
      "loss:0.336349  [51264/100000]\n",
      "loss:0.228054  [57664/100000]\n",
      "loss:0.226541  [64064/100000]\n",
      "loss:0.333538  [70464/100000]\n",
      "loss:0.289454  [76864/100000]\n",
      "loss:0.261543  [83264/100000]\n",
      "loss:0.228298  [89664/100000]\n",
      "loss:0.233951  [96064/100000]\n",
      "Test\n",
      " Avg Loss:1.808245  Accuracy:65.8\n",
      "Epoch76----------------\n",
      "\n",
      "loss:0.117028  [   64/100000]\n",
      "loss:0.272159  [ 6464/100000]\n",
      "loss:0.193962  [12864/100000]\n",
      "loss:0.188299  [19264/100000]\n",
      "loss:0.124925  [25664/100000]\n",
      "loss:0.319131  [32064/100000]\n",
      "loss:0.245568  [38464/100000]\n",
      "loss:0.166888  [44864/100000]\n",
      "loss:0.401097  [51264/100000]\n",
      "loss:0.210816  [57664/100000]\n",
      "loss:0.230689  [64064/100000]\n",
      "loss:0.254787  [70464/100000]\n",
      "loss:0.179299  [76864/100000]\n",
      "loss:0.270088  [83264/100000]\n",
      "loss:0.261857  [89664/100000]\n",
      "loss:0.270514  [96064/100000]\n",
      "Test\n",
      " Avg Loss:1.857319  Accuracy:65.4\n",
      "Epoch77----------------\n",
      "\n",
      "loss:0.104065  [   64/100000]\n",
      "loss:0.106039  [ 6464/100000]\n",
      "loss:0.144032  [12864/100000]\n",
      "loss:0.147910  [19264/100000]\n",
      "loss:0.131224  [25664/100000]\n",
      "loss:0.263617  [32064/100000]\n",
      "loss:0.083207  [38464/100000]\n",
      "loss:0.308790  [44864/100000]\n",
      "loss:0.400456  [51264/100000]\n",
      "loss:0.143969  [57664/100000]\n",
      "loss:0.178356  [64064/100000]\n",
      "loss:0.386923  [70464/100000]\n",
      "loss:0.211627  [76864/100000]\n",
      "loss:0.416205  [83264/100000]\n",
      "loss:0.150413  [89664/100000]\n",
      "loss:0.240140  [96064/100000]\n",
      "Test\n",
      " Avg Loss:1.869743  Accuracy:65.2\n",
      "Epoch78----------------\n",
      "\n",
      "loss:0.172649  [   64/100000]\n",
      "loss:0.202802  [ 6464/100000]\n",
      "loss:0.176913  [12864/100000]\n",
      "loss:0.160024  [19264/100000]\n",
      "loss:0.186722  [25664/100000]\n",
      "loss:0.326889  [32064/100000]\n",
      "loss:0.268099  [38464/100000]\n",
      "loss:0.149226  [44864/100000]\n",
      "loss:0.514740  [51264/100000]\n",
      "loss:0.205644  [57664/100000]\n",
      "loss:0.214091  [64064/100000]\n",
      "loss:0.254387  [70464/100000]\n",
      "loss:0.194725  [76864/100000]\n",
      "loss:0.313707  [83264/100000]\n",
      "loss:0.287562  [89664/100000]\n",
      "loss:0.285699  [96064/100000]\n",
      "Test\n",
      " Avg Loss:1.999642  Accuracy:64.4\n",
      "Epoch79----------------\n",
      "\n",
      "loss:0.145723  [   64/100000]\n",
      "loss:0.322221  [ 6464/100000]\n",
      "loss:0.234598  [12864/100000]\n",
      "loss:0.117866  [19264/100000]\n",
      "loss:0.124642  [25664/100000]\n",
      "loss:0.281668  [32064/100000]\n",
      "loss:0.205896  [38464/100000]\n",
      "loss:0.215422  [44864/100000]\n",
      "loss:0.349527  [51264/100000]\n",
      "loss:0.268110  [57664/100000]\n",
      "loss:0.209277  [64064/100000]\n",
      "loss:0.256608  [70464/100000]\n",
      "loss:0.203868  [76864/100000]\n",
      "loss:0.285583  [83264/100000]\n",
      "loss:0.145403  [89664/100000]\n",
      "loss:0.347787  [96064/100000]\n",
      "Test\n",
      " Avg Loss:1.992752  Accuracy:64.7\n",
      "Epoch80----------------\n",
      "\n",
      "loss:0.196690  [   64/100000]\n",
      "loss:0.248212  [ 6464/100000]\n",
      "loss:0.275866  [12864/100000]\n",
      "loss:0.241404  [19264/100000]\n",
      "loss:0.116300  [25664/100000]\n",
      "loss:0.512605  [32064/100000]\n",
      "loss:0.300521  [38464/100000]\n",
      "loss:0.175009  [44864/100000]\n",
      "loss:0.376020  [51264/100000]\n",
      "loss:0.153397  [57664/100000]\n",
      "loss:0.219760  [64064/100000]\n",
      "loss:0.210866  [70464/100000]\n",
      "loss:0.242897  [76864/100000]\n",
      "loss:0.305684  [83264/100000]\n",
      "loss:0.205835  [89664/100000]\n",
      "loss:0.276451  [96064/100000]\n",
      "Test\n",
      " Avg Loss:1.985034  Accuracy:65.0\n",
      "Epoch81----------------\n",
      "\n",
      "loss:0.262139  [   64/100000]\n",
      "loss:0.157628  [ 6464/100000]\n",
      "loss:0.142855  [12864/100000]\n",
      "loss:0.264173  [19264/100000]\n",
      "loss:0.143237  [25664/100000]\n",
      "loss:0.199509  [32064/100000]\n",
      "loss:0.150094  [38464/100000]\n",
      "loss:0.372955  [44864/100000]\n",
      "loss:0.408767  [51264/100000]\n",
      "loss:0.224972  [57664/100000]\n",
      "loss:0.133069  [64064/100000]\n",
      "loss:0.356082  [70464/100000]\n",
      "loss:0.171402  [76864/100000]\n",
      "loss:0.337012  [83264/100000]\n",
      "loss:0.209972  [89664/100000]\n",
      "loss:0.234695  [96064/100000]\n",
      "Test\n",
      " Avg Loss:1.861258  Accuracy:65.6\n",
      "Epoch82----------------\n",
      "\n",
      "loss:0.285738  [   64/100000]\n",
      "loss:0.256475  [ 6464/100000]\n",
      "loss:0.216509  [12864/100000]\n",
      "loss:0.134961  [19264/100000]\n",
      "loss:0.187533  [25664/100000]\n",
      "loss:0.443982  [32064/100000]\n",
      "loss:0.213565  [38464/100000]\n",
      "loss:0.161102  [44864/100000]\n",
      "loss:0.379130  [51264/100000]\n",
      "loss:0.205793  [57664/100000]\n",
      "loss:0.183734  [64064/100000]\n",
      "loss:0.345229  [70464/100000]\n",
      "loss:0.290833  [76864/100000]\n",
      "loss:0.326369  [83264/100000]\n",
      "loss:0.125083  [89664/100000]\n",
      "loss:0.228321  [96064/100000]\n",
      "Test\n",
      " Avg Loss:1.943698  Accuracy:65.4\n",
      "Epoch83----------------\n",
      "\n",
      "loss:0.214900  [   64/100000]\n",
      "loss:0.209855  [ 6464/100000]\n",
      "loss:0.183132  [12864/100000]\n",
      "loss:0.200233  [19264/100000]\n",
      "loss:0.126960  [25664/100000]\n",
      "loss:0.329585  [32064/100000]\n",
      "loss:0.074066  [38464/100000]\n",
      "loss:0.158343  [44864/100000]\n",
      "loss:0.380667  [51264/100000]\n",
      "loss:0.230239  [57664/100000]\n",
      "loss:0.101405  [64064/100000]\n",
      "loss:0.225550  [70464/100000]\n",
      "loss:0.157878  [76864/100000]\n",
      "loss:0.297226  [83264/100000]\n",
      "loss:0.230069  [89664/100000]\n",
      "loss:0.232980  [96064/100000]\n",
      "Test\n",
      " Avg Loss:2.031941  Accuracy:64.0\n",
      "Epoch84----------------\n",
      "\n",
      "loss:0.161112  [   64/100000]\n",
      "loss:0.218468  [ 6464/100000]\n",
      "loss:0.289748  [12864/100000]\n",
      "loss:0.194375  [19264/100000]\n",
      "loss:0.124002  [25664/100000]\n",
      "loss:0.255193  [32064/100000]\n",
      "loss:0.200879  [38464/100000]\n",
      "loss:0.168955  [44864/100000]\n",
      "loss:0.380871  [51264/100000]\n",
      "loss:0.208206  [57664/100000]\n",
      "loss:0.328619  [64064/100000]\n",
      "loss:0.272622  [70464/100000]\n",
      "loss:0.189641  [76864/100000]\n",
      "loss:0.304470  [83264/100000]\n",
      "loss:0.265382  [89664/100000]\n",
      "loss:0.231461  [96064/100000]\n",
      "Test\n",
      " Avg Loss:2.046716  Accuracy:64.3\n",
      "Epoch85----------------\n",
      "\n",
      "loss:0.188304  [   64/100000]\n",
      "loss:0.232051  [ 6464/100000]\n",
      "loss:0.294357  [12864/100000]\n",
      "loss:0.152005  [19264/100000]\n",
      "loss:0.232910  [25664/100000]\n",
      "loss:0.355151  [32064/100000]\n",
      "loss:0.257964  [38464/100000]\n",
      "loss:0.240759  [44864/100000]\n",
      "loss:0.321101  [51264/100000]\n",
      "loss:0.230654  [57664/100000]\n",
      "loss:0.178967  [64064/100000]\n",
      "loss:0.154357  [70464/100000]\n",
      "loss:0.127409  [76864/100000]\n",
      "loss:0.279008  [83264/100000]\n",
      "loss:0.176856  [89664/100000]\n",
      "loss:0.256172  [96064/100000]\n",
      "Test\n",
      " Avg Loss:1.989234  Accuracy:64.2\n",
      "Epoch86----------------\n",
      "\n",
      "loss:0.182641  [   64/100000]\n",
      "loss:0.273990  [ 6464/100000]\n",
      "loss:0.210244  [12864/100000]\n",
      "loss:0.185955  [19264/100000]\n",
      "loss:0.158780  [25664/100000]\n",
      "loss:0.499299  [32064/100000]\n",
      "loss:0.131023  [38464/100000]\n",
      "loss:0.160093  [44864/100000]\n",
      "loss:0.325993  [51264/100000]\n",
      "loss:0.186481  [57664/100000]\n",
      "loss:0.107515  [64064/100000]\n",
      "loss:0.197390  [70464/100000]\n",
      "loss:0.260143  [76864/100000]\n",
      "loss:0.244684  [83264/100000]\n",
      "loss:0.136019  [89664/100000]\n",
      "loss:0.306408  [96064/100000]\n",
      "Test\n",
      " Avg Loss:1.953525  Accuracy:65.1\n",
      "Epoch87----------------\n",
      "\n",
      "loss:0.226831  [   64/100000]\n",
      "loss:0.197791  [ 6464/100000]\n",
      "loss:0.160528  [12864/100000]\n",
      "loss:0.246099  [19264/100000]\n",
      "loss:0.110480  [25664/100000]\n",
      "loss:0.294850  [32064/100000]\n",
      "loss:0.125156  [38464/100000]\n",
      "loss:0.178896  [44864/100000]\n",
      "loss:0.285449  [51264/100000]\n",
      "loss:0.313886  [57664/100000]\n",
      "loss:0.163615  [64064/100000]\n",
      "loss:0.209346  [70464/100000]\n",
      "loss:0.201737  [76864/100000]\n",
      "loss:0.280552  [83264/100000]\n",
      "loss:0.270478  [89664/100000]\n",
      "loss:0.280752  [96064/100000]\n",
      "Test\n",
      " Avg Loss:1.937954  Accuracy:65.0\n",
      "Epoch88----------------\n",
      "\n",
      "loss:0.147042  [   64/100000]\n",
      "loss:0.163595  [ 6464/100000]\n",
      "loss:0.136049  [12864/100000]\n",
      "loss:0.189335  [19264/100000]\n",
      "loss:0.169098  [25664/100000]\n",
      "loss:0.261206  [32064/100000]\n",
      "loss:0.191699  [38464/100000]\n",
      "loss:0.209915  [44864/100000]\n",
      "loss:0.359648  [51264/100000]\n",
      "loss:0.155718  [57664/100000]\n",
      "loss:0.178350  [64064/100000]\n",
      "loss:0.266464  [70464/100000]\n",
      "loss:0.216068  [76864/100000]\n",
      "loss:0.203735  [83264/100000]\n",
      "loss:0.244108  [89664/100000]\n",
      "loss:0.237132  [96064/100000]\n",
      "Test\n",
      " Avg Loss:1.993090  Accuracy:65.0\n",
      "Epoch89----------------\n",
      "\n",
      "loss:0.158421  [   64/100000]\n",
      "loss:0.162482  [ 6464/100000]\n",
      "loss:0.169996  [12864/100000]\n",
      "loss:0.179378  [19264/100000]\n",
      "loss:0.111065  [25664/100000]\n",
      "loss:0.348071  [32064/100000]\n",
      "loss:0.173426  [38464/100000]\n",
      "loss:0.338926  [44864/100000]\n",
      "loss:0.293359  [51264/100000]\n",
      "loss:0.173445  [57664/100000]\n",
      "loss:0.102871  [64064/100000]\n",
      "loss:0.189580  [70464/100000]\n",
      "loss:0.113519  [76864/100000]\n",
      "loss:0.319302  [83264/100000]\n",
      "loss:0.145691  [89664/100000]\n",
      "loss:0.295801  [96064/100000]\n",
      "Test\n",
      " Avg Loss:2.080655  Accuracy:64.5\n",
      "Epoch90----------------\n",
      "\n",
      "loss:0.163236  [   64/100000]\n",
      "loss:0.231674  [ 6464/100000]\n",
      "loss:0.145714  [12864/100000]\n",
      "loss:0.132969  [19264/100000]\n",
      "loss:0.190663  [25664/100000]\n",
      "loss:0.505352  [32064/100000]\n",
      "loss:0.143818  [38464/100000]\n",
      "loss:0.156565  [44864/100000]\n",
      "loss:0.559475  [51264/100000]\n",
      "loss:0.145438  [57664/100000]\n",
      "loss:0.088165  [64064/100000]\n",
      "loss:0.131773  [70464/100000]\n",
      "loss:0.158044  [76864/100000]\n",
      "loss:0.225839  [83264/100000]\n",
      "loss:0.165872  [89664/100000]\n",
      "loss:0.242519  [96064/100000]\n",
      "Test\n",
      " Avg Loss:2.120238  Accuracy:64.0\n",
      "Epoch91----------------\n",
      "\n",
      "loss:0.109846  [   64/100000]\n",
      "loss:0.244629  [ 6464/100000]\n",
      "loss:0.174243  [12864/100000]\n",
      "loss:0.215040  [19264/100000]\n",
      "loss:0.267264  [25664/100000]\n",
      "loss:0.296463  [32064/100000]\n",
      "loss:0.215997  [38464/100000]\n",
      "loss:0.153807  [44864/100000]\n",
      "loss:0.454808  [51264/100000]\n",
      "loss:0.210461  [57664/100000]\n",
      "loss:0.179387  [64064/100000]\n",
      "loss:0.200942  [70464/100000]\n",
      "loss:0.146502  [76864/100000]\n",
      "loss:0.292521  [83264/100000]\n",
      "loss:0.167608  [89664/100000]\n",
      "loss:0.239748  [96064/100000]\n",
      "Test\n",
      " Avg Loss:2.163966  Accuracy:63.8\n",
      "Epoch92----------------\n",
      "\n",
      "loss:0.171101  [   64/100000]\n",
      "loss:0.232972  [ 6464/100000]\n",
      "loss:0.233008  [12864/100000]\n",
      "loss:0.119336  [19264/100000]\n",
      "loss:0.089556  [25664/100000]\n",
      "loss:0.270227  [32064/100000]\n",
      "loss:0.154167  [38464/100000]\n",
      "loss:0.196380  [44864/100000]\n",
      "loss:0.261059  [51264/100000]\n",
      "loss:0.240682  [57664/100000]\n",
      "loss:0.244793  [64064/100000]\n",
      "loss:0.359073  [70464/100000]\n",
      "loss:0.089367  [76864/100000]\n",
      "loss:0.309163  [83264/100000]\n",
      "loss:0.177914  [89664/100000]\n",
      "loss:0.192992  [96064/100000]\n",
      "Test\n",
      " Avg Loss:2.299021  Accuracy:63.2\n",
      "Epoch93----------------\n",
      "\n",
      "loss:0.154824  [   64/100000]\n",
      "loss:0.321377  [ 6464/100000]\n",
      "loss:0.171944  [12864/100000]\n",
      "loss:0.089344  [19264/100000]\n",
      "loss:0.190561  [25664/100000]\n",
      "loss:0.285262  [32064/100000]\n",
      "loss:0.177439  [38464/100000]\n",
      "loss:0.215339  [44864/100000]\n",
      "loss:0.225709  [51264/100000]\n",
      "loss:0.186209  [57664/100000]\n",
      "loss:0.095545  [64064/100000]\n",
      "loss:0.209546  [70464/100000]\n",
      "loss:0.178429  [76864/100000]\n",
      "loss:0.332733  [83264/100000]\n",
      "loss:0.214148  [89664/100000]\n",
      "loss:0.178147  [96064/100000]\n",
      "Test\n",
      " Avg Loss:2.207289  Accuracy:64.1\n",
      "Epoch94----------------\n",
      "\n",
      "loss:0.193229  [   64/100000]\n",
      "loss:0.156128  [ 6464/100000]\n",
      "loss:0.213187  [12864/100000]\n",
      "loss:0.124714  [19264/100000]\n",
      "loss:0.115288  [25664/100000]\n",
      "loss:0.452052  [32064/100000]\n",
      "loss:0.101579  [38464/100000]\n",
      "loss:0.130141  [44864/100000]\n",
      "loss:0.252815  [51264/100000]\n",
      "loss:0.201485  [57664/100000]\n",
      "loss:0.131076  [64064/100000]\n",
      "loss:0.314031  [70464/100000]\n",
      "loss:0.149226  [76864/100000]\n",
      "loss:0.368841  [83264/100000]\n",
      "loss:0.145124  [89664/100000]\n",
      "loss:0.278997  [96064/100000]\n",
      "Test\n",
      " Avg Loss:2.051225  Accuracy:64.9\n",
      "Epoch95----------------\n",
      "\n",
      "loss:0.153808  [   64/100000]\n",
      "loss:0.181083  [ 6464/100000]\n",
      "loss:0.156467  [12864/100000]\n",
      "loss:0.158628  [19264/100000]\n",
      "loss:0.158710  [25664/100000]\n",
      "loss:0.208280  [32064/100000]\n",
      "loss:0.244679  [38464/100000]\n",
      "loss:0.173079  [44864/100000]\n",
      "loss:0.268822  [51264/100000]\n",
      "loss:0.199167  [57664/100000]\n",
      "loss:0.103872  [64064/100000]\n",
      "loss:0.194284  [70464/100000]\n",
      "loss:0.188150  [76864/100000]\n",
      "loss:0.238384  [83264/100000]\n",
      "loss:0.113909  [89664/100000]\n",
      "loss:0.179108  [96064/100000]\n",
      "Test\n",
      " Avg Loss:2.193474  Accuracy:64.4\n",
      "Epoch96----------------\n",
      "\n",
      "loss:0.147058  [   64/100000]\n",
      "loss:0.185908  [ 6464/100000]\n",
      "loss:0.255644  [12864/100000]\n",
      "loss:0.268960  [19264/100000]\n",
      "loss:0.148229  [25664/100000]\n",
      "loss:0.332769  [32064/100000]\n",
      "loss:0.200783  [38464/100000]\n",
      "loss:0.212602  [44864/100000]\n",
      "loss:0.327530  [51264/100000]\n",
      "loss:0.176609  [57664/100000]\n",
      "loss:0.148849  [64064/100000]\n",
      "loss:0.296681  [70464/100000]\n",
      "loss:0.185237  [76864/100000]\n",
      "loss:0.217330  [83264/100000]\n",
      "loss:0.177481  [89664/100000]\n",
      "loss:0.177612  [96064/100000]\n",
      "Test\n",
      " Avg Loss:2.034355  Accuracy:65.8\n",
      "Epoch97----------------\n",
      "\n",
      "loss:0.195266  [   64/100000]\n",
      "loss:0.145455  [ 6464/100000]\n",
      "loss:0.168866  [12864/100000]\n",
      "loss:0.150688  [19264/100000]\n",
      "loss:0.064282  [25664/100000]\n",
      "loss:0.278994  [32064/100000]\n",
      "loss:0.182713  [38464/100000]\n",
      "loss:0.152343  [44864/100000]\n",
      "loss:0.458960  [51264/100000]\n",
      "loss:0.159089  [57664/100000]\n",
      "loss:0.145926  [64064/100000]\n",
      "loss:0.125725  [70464/100000]\n",
      "loss:0.168689  [76864/100000]\n",
      "loss:0.215166  [83264/100000]\n",
      "loss:0.207565  [89664/100000]\n",
      "loss:0.220108  [96064/100000]\n",
      "Test\n",
      " Avg Loss:2.262143  Accuracy:63.9\n",
      "Epoch98----------------\n",
      "\n",
      "loss:0.128333  [   64/100000]\n",
      "loss:0.206825  [ 6464/100000]\n",
      "loss:0.205099  [12864/100000]\n",
      "loss:0.135404  [19264/100000]\n",
      "loss:0.166564  [25664/100000]\n",
      "loss:0.275902  [32064/100000]\n",
      "loss:0.085308  [38464/100000]\n",
      "loss:0.121796  [44864/100000]\n",
      "loss:0.418274  [51264/100000]\n",
      "loss:0.148931  [57664/100000]\n",
      "loss:0.074677  [64064/100000]\n",
      "loss:0.152177  [70464/100000]\n",
      "loss:0.176772  [76864/100000]\n",
      "loss:0.362654  [83264/100000]\n",
      "loss:0.133144  [89664/100000]\n",
      "loss:0.244092  [96064/100000]\n",
      "Test\n",
      " Avg Loss:2.191239  Accuracy:65.0\n",
      "Epoch99----------------\n",
      "\n",
      "loss:0.199819  [   64/100000]\n",
      "loss:0.143899  [ 6464/100000]\n",
      "loss:0.151129  [12864/100000]\n",
      "loss:0.151804  [19264/100000]\n",
      "loss:0.291843  [25664/100000]\n",
      "loss:0.221160  [32064/100000]\n",
      "loss:0.082693  [38464/100000]\n",
      "loss:0.106500  [44864/100000]\n",
      "loss:0.284455  [51264/100000]\n",
      "loss:0.166886  [57664/100000]\n",
      "loss:0.120110  [64064/100000]\n",
      "loss:0.173074  [70464/100000]\n",
      "loss:0.187401  [76864/100000]\n",
      "loss:0.236681  [83264/100000]\n",
      "loss:0.148142  [89664/100000]\n",
      "loss:0.261435  [96064/100000]\n",
      "Test\n",
      " Avg Loss:2.354326  Accuracy:63.4\n",
      "Epoch100----------------\n",
      "\n",
      "loss:0.121516  [   64/100000]\n",
      "loss:0.127890  [ 6464/100000]\n",
      "loss:0.207259  [12864/100000]\n",
      "loss:0.071774  [19264/100000]\n",
      "loss:0.103693  [25664/100000]\n",
      "loss:0.274711  [32064/100000]\n",
      "loss:0.261824  [38464/100000]\n",
      "loss:0.151452  [44864/100000]\n",
      "loss:0.348831  [51264/100000]\n",
      "loss:0.143220  [57664/100000]\n",
      "loss:0.151986  [64064/100000]\n",
      "loss:0.281076  [70464/100000]\n",
      "loss:0.112107  [76864/100000]\n",
      "loss:0.342799  [83264/100000]\n",
      "loss:0.153030  [89664/100000]\n",
      "loss:0.301512  [96064/100000]\n",
      "Test\n",
      " Avg Loss:2.263954  Accuracy:64.5\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "epochs=100\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch{t+1}----------------\\n\")\n",
    "    train(model, train_dataloader, loss_fn, optimizer)\n",
    "    test(model, test_dataloader, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a tensorflow keras model for classification\n",
    "import tensorflow as tf\n",
    "from tensorflow.data import Dataset\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement regularization\n",
    "\n",
    "data_normalizer=layers.Normalization()\n",
    "\n",
    "def setup_l2_model():\n",
    "    inputs=Input(shape=(200,))\n",
    "    x=data_normalizer(inputs)\n",
    "    x=layers.Dense(128, activation='relu', kernel_regularizer=regularizers.L2(0.01))(x)\n",
    "    x=layers.Dense(128, activation='relu', kernel_regularizer=regularizers.L2(0.01))(x)\n",
    "    x=layers.Dense(128, activation='relu', kernel_regularizer=regularizers.L2(0.01))(x)\n",
    "    outputs=layers.Dense(10, activation='softmax', kernel_regularizer=regularizers.L2(0.01))(x)\n",
    "    model=Model(inputs, outputs)\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "def compile_model(model):\n",
    "    loss_fn=losses.SparseCategoricalCrossentropy()\n",
    "    optimizer=optimizers.legacy.Adam(learning_rate=1e-3)\n",
    "    metric=[metrics.SparseCategoricalAccuracy()]\n",
    "    model.compile(optimizer=optimizer, loss=loss_fn, metrics=metric)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 200)]             0         \n",
      "                                                                 \n",
      " normalization_2 (Normalizat  (None, 200)              401       \n",
      " ion)                                                            \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 128)               25728     \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 60,443\n",
      "Trainable params: 60,042\n",
      "Non-trainable params: 401\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "data_normalizer.adapt(x_train)\n",
    "l2_model=setup_l2_model()\n",
    "l2model=compile_model(l2_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1563/1563 [==============================] - 2s 939us/step - loss: 1.5114 - sparse_categorical_accuracy: 0.7137 - val_loss: 0.9833 - val_sparse_categorical_accuracy: 0.8130\n",
      "Epoch 2/10\n",
      "1563/1563 [==============================] - 1s 867us/step - loss: 0.9346 - sparse_categorical_accuracy: 0.8315 - val_loss: 0.9226 - val_sparse_categorical_accuracy: 0.8290\n",
      "Epoch 3/10\n",
      "1563/1563 [==============================] - 1s 860us/step - loss: 0.8864 - sparse_categorical_accuracy: 0.8464 - val_loss: 0.8857 - val_sparse_categorical_accuracy: 0.8432\n",
      "Epoch 4/10\n",
      "1563/1563 [==============================] - 1s 901us/step - loss: 0.8645 - sparse_categorical_accuracy: 0.8532 - val_loss: 0.8377 - val_sparse_categorical_accuracy: 0.8776\n",
      "Epoch 5/10\n",
      "1563/1563 [==============================] - 1s 881us/step - loss: 0.8476 - sparse_categorical_accuracy: 0.8602 - val_loss: 0.8334 - val_sparse_categorical_accuracy: 0.8690\n",
      "Epoch 6/10\n",
      "1563/1563 [==============================] - 1s 881us/step - loss: 0.8387 - sparse_categorical_accuracy: 0.8647 - val_loss: 0.8226 - val_sparse_categorical_accuracy: 0.8755\n",
      "Epoch 7/10\n",
      "1563/1563 [==============================] - 1s 893us/step - loss: 0.8312 - sparse_categorical_accuracy: 0.8683 - val_loss: 0.8225 - val_sparse_categorical_accuracy: 0.8703\n",
      "Epoch 8/10\n",
      "1563/1563 [==============================] - 1s 877us/step - loss: 0.8259 - sparse_categorical_accuracy: 0.8703 - val_loss: 0.8468 - val_sparse_categorical_accuracy: 0.8459\n",
      "Epoch 9/10\n",
      "1563/1563 [==============================] - 1s 945us/step - loss: 0.8195 - sparse_categorical_accuracy: 0.8724 - val_loss: 0.8086 - val_sparse_categorical_accuracy: 0.8775\n",
      "Epoch 10/10\n",
      "1563/1563 [==============================] - 1s 954us/step - loss: 0.8139 - sparse_categorical_accuracy: 0.8767 - val_loss: 0.8012 - val_sparse_categorical_accuracy: 0.8832\n"
     ]
    }
   ],
   "source": [
    "\n",
    "history=l2_model.fit(x_train, y_train, batch_size=64, epochs=10, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 355us/step\n",
      "Real targets:[4 1 1 8 5 0 5 3 8 9 8 5 1 4 6 2 0 6 9 5]\n",
      "\n",
      "Pred targets:[3 1 1 8 5 0 5 3 8 9 8 5 1 4 6 2 0 7 9 5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_hat=l2_model.predict(x_test)\n",
    "y_hat_classes=y_hat.argmax(1)\n",
    "num_obs=20\n",
    "print(f\"Real targets:{y_hat_classes[0:num_obs]}\\n\")\n",
    "print(f\"Pred targets:{y_test[0:num_obs]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement a regulatization using a dropout approach\n",
    "\n",
    "data_normalizer=layers.Normalization()\n",
    "\n",
    "def setup_dropout_model():\n",
    "    inputs=Input(shape=(200,))\n",
    "    x=data_normalizer(inputs)\n",
    "    x=layers.Dense(128, activation='relu')(x)\n",
    "    x=layers.Dropout(0.2)(x)\n",
    "    x=layers.Dense(128, activation='relu')(x)\n",
    "    x=layers.Dropout(0.2)(x)\n",
    "    x=layers.Dense(128, activation='relu')(x)\n",
    "    x=layers.Dropout(0.2)(x)\n",
    "    outputs=layers.Dense(10, activation='softmax')(x)\n",
    "    model=Model(inputs, outputs)\n",
    "    model.summary()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 200)]             0         \n",
      "                                                                 \n",
      " normalization_3 (Normalizat  (None, 200)              401       \n",
      " ion)                                                            \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 128)               25728     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 60,443\n",
      "Trainable params: 60,042\n",
      "Non-trainable params: 401\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "data_normalizer.adapt(x_train)\n",
    "dropout_model=setup_dropout_model()\n",
    "dropout_model=compile_model(dropout_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.0834 - sparse_categorical_accuracy: 0.5343 - val_loss: 0.5033 - val_sparse_categorical_accuracy: 0.7909\n",
      "Epoch 2/10\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 0.6154 - sparse_categorical_accuracy: 0.7269 - val_loss: 0.4151 - val_sparse_categorical_accuracy: 0.8229\n",
      "Epoch 3/10\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 0.5184 - sparse_categorical_accuracy: 0.7762 - val_loss: 0.3963 - val_sparse_categorical_accuracy: 0.8265\n",
      "Epoch 4/10\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 0.4657 - sparse_categorical_accuracy: 0.8015 - val_loss: 0.3404 - val_sparse_categorical_accuracy: 0.8583\n",
      "Epoch 5/10\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 0.4227 - sparse_categorical_accuracy: 0.8205 - val_loss: 0.3156 - val_sparse_categorical_accuracy: 0.8629\n",
      "Epoch 6/10\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 0.4009 - sparse_categorical_accuracy: 0.8324 - val_loss: 0.3008 - val_sparse_categorical_accuracy: 0.8726\n",
      "Epoch 7/10\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 0.3829 - sparse_categorical_accuracy: 0.8404 - val_loss: 0.3043 - val_sparse_categorical_accuracy: 0.8727\n",
      "Epoch 8/10\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 0.3646 - sparse_categorical_accuracy: 0.8484 - val_loss: 0.2972 - val_sparse_categorical_accuracy: 0.8689\n",
      "Epoch 9/10\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 0.3526 - sparse_categorical_accuracy: 0.8532 - val_loss: 0.2868 - val_sparse_categorical_accuracy: 0.8771\n",
      "Epoch 10/10\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 0.3411 - sparse_categorical_accuracy: 0.8588 - val_loss: 0.2762 - val_sparse_categorical_accuracy: 0.8834\n"
     ]
    }
   ],
   "source": [
    "history=dropout_model.fit(x_train, y_train, batch_size=64, epochs=10, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 371us/step\n",
      "Real targets:[3 1 1 9 5 0 5 2 8 9 9 5 1 4 6 2 0 6 9 5]\n",
      "\n",
      "Pred targets:[3 1 1 8 5 0 5 3 8 9 8 5 1 4 6 2 0 7 9 5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_hat=dropout_model.predict(x_test)\n",
    "y_hat_classes=y_hat.argmax(1)\n",
    "num_obs=20\n",
    "print(f\"Real targets:{y_hat_classes[0:num_obs]}\\n\")\n",
    "print(f\"Pred targets:{y_test[0:num_obs]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pruning a neural network\n",
    "import tensorflow_model_optimization as tfmot\n",
    "from tensorflow.keras import Sequential\n",
    "import sys\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "%load_ext tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_normalizer=layers.Normalization()\n",
    "\n",
    "def functional_model():\n",
    "    inputs=Input(shape=(200,))\n",
    "    x=data_normalizer(inputs)\n",
    "    x=layers.Dense(128, activation='relu', kernel_regularizer=regularizers.L2(0.01))(x)\n",
    "    x=layers.Dense(128, activation='relu', kernel_regularizer=regularizers.L2(0.01))(x)\n",
    "    x=layers.Dense(128, activation='relu', kernel_regularizer=regularizers.L2(0.01))(x)\n",
    "    x=layers.Dense(128, activation='relu', kernel_regularizer=regularizers.L2(0.01))(x)\n",
    "    outputs=layers.Dense(10, activation='softmax', kernel_regularizer=regularizers.L2(0.01))(x)\n",
    "    model=Model(inputs, outputs)\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "def sequential_model():\n",
    "    model = Sequential([\n",
    "    layers.InputLayer(input_shape=(200,)),\n",
    "    data_normalizer,\n",
    "    layers.Dense(128, activation='relu', kernel_regularizer=regularizers.L2(0.01)),\n",
    "    layers.Dense(128, activation='relu', kernel_regularizer=regularizers.L2(0.01)),\n",
    "    layers.Dense(128, activation='relu', kernel_regularizer=regularizers.L2(0.01)),\n",
    "    layers.Dense(10, activation='softmax', kernel_regularizer=regularizers.L2(0.01))\n",
    "    ])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "def setup_model(model_type:str=\"sequential\"):\n",
    "    if model_type==\"sequential\":\n",
    "        model=sequential_model()\n",
    "    elif model_type==\"functional\":\n",
    "        model=functional_model()\n",
    "    else:\n",
    "        print(\"Error: the model type is not recognized. Enter model_type='sequential' or 'functional'\")\n",
    "        sys.exit()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_12 (InputLayer)       [(None, 200)]             0         \n",
      "                                                                 \n",
      " normalization_8 (Normalizat  (None, 200)              401       \n",
      " ion)                                                            \n",
      "                                                                 \n",
      " dense_48 (Dense)            (None, 128)               25728     \n",
      "                                                                 \n",
      " dense_49 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_50 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_51 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_52 (Dense)            (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 76,955\n",
      "Trainable params: 76,554\n",
      "Non-trainable params: 401\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "4688/4688 [==============================] - 6s 1ms/step - loss: 1.0897 - sparse_categorical_accuracy: 0.7898 - val_loss: 0.8225 - val_sparse_categorical_accuracy: 0.8548\n",
      "Epoch 2/10\n",
      "4688/4688 [==============================] - 5s 1ms/step - loss: 0.8017 - sparse_categorical_accuracy: 0.8527 - val_loss: 0.7597 - val_sparse_categorical_accuracy: 0.8745\n",
      "Epoch 3/10\n",
      "4688/4688 [==============================] - 5s 1ms/step - loss: 0.7637 - sparse_categorical_accuracy: 0.8664 - val_loss: 0.7544 - val_sparse_categorical_accuracy: 0.8711\n",
      "Epoch 4/10\n",
      "4688/4688 [==============================] - 5s 1ms/step - loss: 0.7408 - sparse_categorical_accuracy: 0.8757 - val_loss: 0.7223 - val_sparse_categorical_accuracy: 0.8859\n",
      "Epoch 5/10\n",
      "4688/4688 [==============================] - 5s 1ms/step - loss: 0.7261 - sparse_categorical_accuracy: 0.8814 - val_loss: 0.7146 - val_sparse_categorical_accuracy: 0.8878\n",
      "Epoch 6/10\n",
      "4688/4688 [==============================] - 5s 1ms/step - loss: 0.7118 - sparse_categorical_accuracy: 0.8876 - val_loss: 0.6880 - val_sparse_categorical_accuracy: 0.9041\n",
      "Epoch 7/10\n",
      "4688/4688 [==============================] - 5s 1ms/step - loss: 0.7019 - sparse_categorical_accuracy: 0.8917 - val_loss: 0.6806 - val_sparse_categorical_accuracy: 0.9062\n",
      "Epoch 8/10\n",
      "4688/4688 [==============================] - 5s 1ms/step - loss: 0.6906 - sparse_categorical_accuracy: 0.8967 - val_loss: 0.7172 - val_sparse_categorical_accuracy: 0.8784\n",
      "Epoch 9/10\n",
      "4688/4688 [==============================] - 6s 1ms/step - loss: 0.6841 - sparse_categorical_accuracy: 0.9003 - val_loss: 0.6956 - val_sparse_categorical_accuracy: 0.8890\n",
      "Epoch 10/10\n",
      "4688/4688 [==============================] - 5s 1ms/step - loss: 0.6783 - sparse_categorical_accuracy: 0.9027 - val_loss: 0.6636 - val_sparse_categorical_accuracy: 0.9120\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x17ba83580>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_normalizer.adapt(x_train)\n",
    "model=compile_model(setup_model(\"functional\"))\n",
    "model.fit(x_train, y_train, batch_size=64, epochs=10, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save pretained weights in the output\n",
    "pretrained_weights=\"output/pretrained_weights.tf\"\n",
    "\n",
    "base_model_file=\"output/base_model.h5\"\n",
    "\n",
    "#_, pretrained_weights = tempfile.mkstemp('.tf')\n",
    "model.save_weights(pretrained_weights)\n",
    "model.save(base_model_file, include_optimizer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of pruned Keras model: 0.97 Mbytes\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"Size of pruned Keras model: %.2f Mbytes\" % (os.path.getsize(base_model_file)/1e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_17 (InputLayer)       [(None, 200)]             0         \n",
      "                                                                 \n",
      " normalization_8 (Normalizat  (None, 200)              401       \n",
      " ion)                                                            \n",
      "                                                                 \n",
      " dense_73 (Dense)            (None, 128)               25728     \n",
      "                                                                 \n",
      " dense_74 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_75 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_76 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_77 (Dense)            (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 76,955\n",
      "Trainable params: 76,554\n",
      "Non-trainable params: 401\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).layer_with_weights-1.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).layer_with_weights-1.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).layer_with_weights-2.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).layer_with_weights-2.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).layer_with_weights-3.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).layer_with_weights-3.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).layer_with_weights-4.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).layer_with_weights-4.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).layer_with_weights-5.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).layer_with_weights-5.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).layer_with_weights-1.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).layer_with_weights-1.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).layer_with_weights-2.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).layer_with_weights-2.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).layer_with_weights-3.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).layer_with_weights-3.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).layer_with_weights-4.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).layer_with_weights-4.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).layer_with_weights-5.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).layer_with_weights-5.bias\n",
      "Model: \"model_16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_17 (InputLayer)       [(None, 200)]             0         \n",
      "                                                                 \n",
      " normalization_8 (Normalizat  (None, 200)              401       \n",
      " ion)                                                            \n",
      "                                                                 \n",
      " prune_low_magnitude_dense_7  (None, 128)              51330     \n",
      " 3 (PruneLowMagnitude)                                           \n",
      "                                                                 \n",
      " prune_low_magnitude_dense_7  (None, 128)              32898     \n",
      " 4 (PruneLowMagnitude)                                           \n",
      "                                                                 \n",
      " prune_low_magnitude_dense_7  (None, 128)              32898     \n",
      " 5 (PruneLowMagnitude)                                           \n",
      "                                                                 \n",
      " prune_low_magnitude_dense_7  (None, 128)              32898     \n",
      " 6 (PruneLowMagnitude)                                           \n",
      "                                                                 \n",
      " prune_low_magnitude_dense_7  (None, 10)               2572      \n",
      " 7 (PruneLowMagnitude)                                           \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 152,997\n",
      "Trainable params: 76,554\n",
      "Non-trainable params: 76,443\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Fine tune model with pruning\n",
    "\n",
    "base_model = setup_model(\"functional\")\n",
    "base_model.load_weights(pretrained_weights)\n",
    "\n",
    "end_step=int(len(x_train)/64)\n",
    "\n",
    "pruning_params = {\n",
    "    'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(\n",
    "        initial_sparsity=0.2,\n",
    "        final_sparsity=0.8,\n",
    "        begin_step=0,\n",
    "        end_step=end_step\n",
    "    ),\n",
    "}\n",
    "\n",
    "def apply_pruning_to_dense(layer):\n",
    "  if isinstance(layer, tf.keras.layers.Dense):\n",
    "    return tfmot.sparsity.keras.prune_low_magnitude(layer, **pruning_params) \n",
    "  return layer\n",
    "\n",
    "model_for_pruning = tf.keras.models.clone_model(\n",
    "    base_model,\n",
    "    clone_function=apply_pruning_to_dense,\n",
    ")\n",
    "\n",
    "model_for_pruning.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_model=compile_model(model_for_pruning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "   1/4688 [..............................] - ETA: 2:06:53 - loss: 0.6773 - sparse_categorical_accuracy: 0.9219WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0072s vs `on_train_batch_end` time: 0.0075s). Check your callbacks.\n",
      "4688/4688 [==============================] - 10s 2ms/step - loss: 0.6736 - sparse_categorical_accuracy: 0.9044 - val_loss: 0.6616 - val_sparse_categorical_accuracy: 0.9105\n",
      "Epoch 2/10\n",
      "4688/4688 [==============================] - 7s 1ms/step - loss: 0.6702 - sparse_categorical_accuracy: 0.9055 - val_loss: 0.6652 - val_sparse_categorical_accuracy: 0.9084\n",
      "Epoch 3/10\n",
      "4688/4688 [==============================] - 7s 2ms/step - loss: 0.6658 - sparse_categorical_accuracy: 0.9070 - val_loss: 0.6662 - val_sparse_categorical_accuracy: 0.9078\n",
      "Epoch 4/10\n",
      "4688/4688 [==============================] - 7s 1ms/step - loss: 0.6620 - sparse_categorical_accuracy: 0.9093 - val_loss: 0.6573 - val_sparse_categorical_accuracy: 0.9116\n",
      "Epoch 5/10\n",
      "4688/4688 [==============================] - 6s 1ms/step - loss: 0.6597 - sparse_categorical_accuracy: 0.9110 - val_loss: 0.6512 - val_sparse_categorical_accuracy: 0.9149\n",
      "Epoch 6/10\n",
      "4688/4688 [==============================] - 6s 1ms/step - loss: 0.6585 - sparse_categorical_accuracy: 0.9108 - val_loss: 0.6502 - val_sparse_categorical_accuracy: 0.9160\n",
      "Epoch 7/10\n",
      "4688/4688 [==============================] - 6s 1ms/step - loss: 0.6560 - sparse_categorical_accuracy: 0.9120 - val_loss: 0.6506 - val_sparse_categorical_accuracy: 0.9128\n",
      "Epoch 8/10\n",
      "4688/4688 [==============================] - 6s 1ms/step - loss: 0.6531 - sparse_categorical_accuracy: 0.9132 - val_loss: 0.6674 - val_sparse_categorical_accuracy: 0.9037\n",
      "Epoch 9/10\n",
      "4688/4688 [==============================] - 6s 1ms/step - loss: 0.6487 - sparse_categorical_accuracy: 0.9155 - val_loss: 0.6363 - val_sparse_categorical_accuracy: 0.9235\n",
      "Epoch 10/10\n",
      "4688/4688 [==============================] - 7s 1ms/step - loss: 0.6467 - sparse_categorical_accuracy: 0.9163 - val_loss: 0.6471 - val_sparse_categorical_accuracy: 0.9119\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x28cab2da0>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_dir = tempfile.mkdtemp()\n",
    "callbacks = [\n",
    "    tfmot.sparsity.keras.UpdatePruningStep(),\n",
    "    # Log sparsity and other metrics in Tensorboard.\n",
    "    tfmot.sparsity.keras.PruningSummaries(log_dir=log_dir)\n",
    "]\n",
    "\n",
    "pruned_model.fit(x_train, y_train, batch_size=64, epochs=10, validation_data=(x_test, y_test), callbacks=callbacks)\n",
    "\n",
    "#docs_infra: no_execute\n",
    "#%tensorboard --logdir={log_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, keras_model_file = tempfile.mkstemp('.h5')\n",
    "\n",
    "# Checkpoint: saving the optimizer is necessary (include_optimizer=True is the default).\n",
    "model_for_pruning.save(keras_model_file, include_optimizer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "model_for_export = tfmot.sparsity.keras.strip_pruning(model_for_pruning)\n",
    "\n",
    "strip_model_file=\"output/strip_pruned_model.h5\"\n",
    "model_for_export.save(strip_model_file, include_optimizer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/4j/fv30hb2s7zl7_5dgpz7hqdb80000gn/T/tmpg19fm6em/assets\n",
      "Saved pruned TFLite model to: output/final_model.tflite\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-12 01:40:40.636376: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "2023-08-12 01:40:40.636655: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "2023-08-12 01:40:40.641851: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /var/folders/4j/fv30hb2s7zl7_5dgpz7hqdb80000gn/T/tmpg19fm6em\n",
      "2023-08-12 01:40:40.643132: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-08-12 01:40:40.643141: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /var/folders/4j/fv30hb2s7zl7_5dgpz7hqdb80000gn/T/tmpg19fm6em\n",
      "2023-08-12 01:40:40.651239: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:353] MLIR V1 optimization pass is not enabled\n",
      "2023-08-12 01:40:40.664468: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-08-12 01:40:40.737729: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /var/folders/4j/fv30hb2s7zl7_5dgpz7hqdb80000gn/T/tmpg19fm6em\n",
      "2023-08-12 01:40:40.748983: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 106649 microseconds.\n",
      "2023-08-12 01:40:40.832262: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model_for_export)\n",
    "pruned_tflite_model = converter.convert()\n",
    "\n",
    "pruned_tflite_file = 'output/final_model.tflite'\n",
    "\n",
    "with open(pruned_tflite_file, 'wb') as f:\n",
    "  f.write(pruned_tflite_model)\n",
    "\n",
    "print('Saved pruned TFLite model to:', pruned_tflite_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/4j/fv30hb2s7zl7_5dgpz7hqdb80000gn/T/tmp0ike5md2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/4j/fv30hb2s7zl7_5dgpz7hqdb80000gn/T/tmp0ike5md2/assets\n",
      "2023-08-12 01:58:54.092976: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "2023-08-12 01:58:54.092992: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "2023-08-12 01:58:54.093268: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /var/folders/4j/fv30hb2s7zl7_5dgpz7hqdb80000gn/T/tmp0ike5md2\n",
      "2023-08-12 01:58:54.095837: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-08-12 01:58:54.095843: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /var/folders/4j/fv30hb2s7zl7_5dgpz7hqdb80000gn/T/tmp0ike5md2\n",
      "2023-08-12 01:58:54.123050: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-08-12 01:58:54.150518: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /var/folders/4j/fv30hb2s7zl7_5dgpz7hqdb80000gn/T/tmp0ike5md2\n",
      "2023-08-12 01:58:54.159593: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 66321 microseconds.\n"
     ]
    }
   ],
   "source": [
    "#  quantization\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model_for_export)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "quantized_and_pruned_tflite_model = converter.convert()\n",
    "\n",
    "quantized_and_pruned_tflite_file = 'output/quantized_model.tflite'\n",
    "\n",
    "with open(quantized_and_pruned_tflite_file, 'wb') as f:\n",
    "  f.write(quantized_and_pruned_tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A helper function to measure the size of the model\n",
    "\n",
    "def get_gzipped_model_size(file):\n",
    "  # Returns size of gzipped model, in bytes.\n",
    "  import os\n",
    "  import zipfile\n",
    "\n",
    "  _, zipped_file = tempfile.mkstemp('.zip')\n",
    "  with zipfile.ZipFile(zipped_file, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
    "    f.write(file)\n",
    "\n",
    "  return os.path.getsize(zipped_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " File size of the base model : 0.88 Mbytes\n",
      " File size of the pruned model : 0.45 Mbytes\n",
      " File size of the strip pruned model : 0.08 Mbytes\n",
      " File size of the tflite pruned model : 0.08 Mbytes\n",
      " File size of the quantized pruned model : 0.01 Mbytes\n"
     ]
    }
   ],
   "source": [
    "print(\" File size of the base model : %.2f Mbytes\"%(get_gzipped_model_size(\"output/base_model.h5\")/1e6))\n",
    "print(\" File size of the pruned model : %.2f Mbytes\"%(get_gzipped_model_size(\"output/keras_model.h5\")/1e6))\n",
    "print(\" File size of the strip pruned model : %.2f Mbytes\"%(get_gzipped_model_size(\"output/strip_pruned_model.h5\")/1e6))\n",
    "print(\" File size of the tflite pruned model : %.2f Mbytes\"%(get_gzipped_model_size(\"output/final_model.tflite\")/1e6))\n",
    "print(\" File size of the quantized pruned model : %.2f Mbytes\"%(get_gzipped_model_size(\"output/quantized_model.tflite\")/1e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of pruned Keras model: 1.29 Mbytes\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "model_for_pruning.save(keras_model_file, include_optimizer=True)\n",
    "\n",
    "print(\"Size of pruned Keras model: %.2f Mbytes\" % (os.path.getsize(keras_model_file)/1e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_15\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_16 (InputLayer)       [(None, 200)]             0         \n",
      "                                                                 \n",
      " normalization_8 (Normalizat  (None, 200)              401       \n",
      " ion)                                                            \n",
      "                                                                 \n",
      " prune_low_magnitude_dense_6  (None, 128)              51330     \n",
      " 8 (PruneLowMagnitude)                                           \n",
      "                                                                 \n",
      " prune_low_magnitude_dense_6  (None, 128)              32898     \n",
      " 9 (PruneLowMagnitude)                                           \n",
      "                                                                 \n",
      " prune_low_magnitude_dense_7  (None, 128)              32898     \n",
      " 0 (PruneLowMagnitude)                                           \n",
      "                                                                 \n",
      " prune_low_magnitude_dense_7  (None, 128)              32898     \n",
      " 1 (PruneLowMagnitude)                                           \n",
      "                                                                 \n",
      " prune_low_magnitude_dense_7  (None, 10)               2572      \n",
      " 2 (PruneLowMagnitude)                                           \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 152,997\n",
      "Trainable params: 76,554\n",
      "Non-trainable params: 76,443\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Deserialize model.\n",
    "with tfmot.sparsity.keras.prune_scope():\n",
    "  loaded_model = tf.keras.models.load_model(keras_model_file)\n",
    "\n",
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 407us/step\n",
      "Real targets:[9 7 4 4 4 5 6 6 0 7 9 0 3 1 3 0 9 9 6 1]\n",
      "\n",
      "Pred targets:[9 7 4 3 4 6 6 6 0 7 9 0 3 1 3 0 9 9 6 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_hat=loaded_model.predict(x_test)\n",
    "y_hat_classes=y_hat.argmax(1)\n",
    "num_obs=20\n",
    "print(f\"Real targets:{y_hat_classes[0:num_obs]}\\n\")\n",
    "print(f\"Pred targets:{y_test[0:num_obs]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the quantization of the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
